{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Discovering Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# we'll use keras for neural networks\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Let's check our software versions\n",
    "print('### Python version: ' + sys.version)\n",
    "print('### Numpy version: ' + np.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('### Tensorflow version: ' + tf.__version__)\n",
    "print('------------')\n",
    "\n",
    "# load our packages / code\n",
    "sys.path.insert(1, '../common/')\n",
    "import utils\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "\n",
    "seed = 42 # deterministic seed\n",
    "np.random.seed(seed) \n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "prop_vec = [24, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to think of Tensorflow? Is it like scikit-learn but for neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not really, think of Tensorflow as a kind of NumPy with additional features (i.e., ability to create computational graphs on tensors, automatically compute derivative, run operations on GPUs). (Tensorflow also has many high-level APIs.)\n",
    "\n",
    "### What are tensors? Well formally they are multilinear maps from vector spaces to reals; but that doesn't matter the point is that tensors can represent scalars, vectors, matrices, etc.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beware that Tensorflow 2.0 is different from Tensorflow 1.0! In this course we'll use Tensorflow 2.0.\n",
    "\n",
    "### Compared to TF 1.0:\n",
    "### - TF 2.0 incorporates Keras as a high-level API\n",
    "### - TF 2.0 does *eager* execution by default!\n",
    "#### In TF 1.0 you would first build the computational graph (construction phase) and then you would execute it in a session (execution phase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we set the seed for Tensorflow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get familiar with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = 7 # a scalar in Python\n",
    "\n",
    "scalar_tf = tf.constant(7) # a TF scalar\n",
    "\n",
    "print(scalar)\n",
    "print(scalar_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just like numpy array, tensors have a shape and dtype property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_np = np.array([3, -5, 9, 1])\n",
    "print(vector_np)\n",
    "\n",
    "vector_tf = tf.constant([3, -5, 9, 1])\n",
    "print(vector_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can get the dtype, shape of tensor. We can also get at the underlying numpy array using numpy()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dtype: ' + str(vector_tf.dtype))\n",
    "print('shape: ' + str(vector_tf.shape))\n",
    "\n",
    "numpy_arr = vector_tf.numpy()\n",
    "print('numpy array: {}, type: {}'.format(str(numpy_arr), type(numpy_arr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also build a tensor out of a numpy array\n",
    "matrix_np = np.array([[3, -7], [0, 9]])\n",
    "matrix_tf = tf.constant(matrix_np)\n",
    "\n",
    "print(matrix_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can construct tensors in similar ways to how we construct some numpy arrays. For example:\n",
    "\n",
    "tf_ones = tf.ones((3,3))\n",
    "print(tf_ones)\n",
    "print()\n",
    "\n",
    "# and\n",
    "\n",
    "tf_unifrand = tf.random.uniform((2, 4))\n",
    "print(tf_unifrand)\n",
    "print()\n",
    "\n",
    "tf_zeros_like_ones = tf.zeros_like(tf_ones)\n",
    "print(tf_zeros_like_ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can check if something is a Tensor. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.is_tensor(matrix_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.is_tensor(matrix_tf.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can also place tensors onto devices. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    matrix_on_gpu0 = tf.identity(matrix_tf) # won't work if you don't have a GPU\n",
    "    \n",
    "print(matrix_on_gpu0.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num CPUs Available: \", len(tf.config.list_physical_devices('CPU')))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can do operations as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([1, 3])\n",
    "y = tf.constant([-1, 2])\n",
    "\n",
    "add_x_y = tf.add(x, y)\n",
    "print(add_x_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 7\n",
    "y = np.array([8, 9])\n",
    "\n",
    "x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.add_n(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((1,3), dtype=np.int32)\n",
    "y = 7.1\n",
    "x+y\n",
    "\n",
    "\n",
    "tf.add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we do x + y?\n",
    "x_plus_y = x + y\n",
    "print(x_plus_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplication by a scalar\n",
    "x_mult_mone = x * -1\n",
    "print(x_mult_mone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elementwise multiplication\n",
    "x_mult_y = x * y\n",
    "# or: x_mult_y = tf.multiply(x,y)\n",
    "print(x_mult_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what about matrix multiplication and similar ops?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.constant([[1, 0, 3], [0, -2, 5]])\n",
    "B = tf.constant([2, -3])\n",
    "\n",
    "print(A.shape)\n",
    "print(B.shape)\n",
    "\n",
    "A_transposed = tf.transpose(A)\n",
    "print(A_transposed.shape)\n",
    "\n",
    "B_reshaped = tf.reshape(B, (-1, 1))\n",
    "\n",
    "print(B_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_T_matrix_mult_B = tf.linalg.matmul(A_transposed, B_reshaped)\n",
    "# or A_transposed @ B_reshaped\n",
    "\n",
    "print(A_T_matrix_mult_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because tensors are immutable, we cannot change their values in place. This seems like it could be a problem because parameters of a model are variables whose values should change frequently.\n",
    "### For this we can use: tf.Variable\n",
    "\n",
    "#### We'll typically use those for model parameters and other variables that need to change often in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's declare a variable\n",
    "# variables in TF represent tensors and you change their values by running operations (ops) on them\n",
    "x = tf.Variable([7, 3], name=\"x\")   # we can name variables (we don't have to, but we can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables also have shape and dtype, etc.\n",
    "print(x.shape, x.dtype, x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you do ops on a variable the result is a tensor not a variable!\n",
    "xsquared = tf.square(x)\n",
    "print(xsquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but variables unlike constant can have their values changed in-place (e.g., using one of the assign*() methods). \n",
    "# For example:\n",
    "x.assign(tf.constant([-1, 0]))\n",
    "print(x)\n",
    "\n",
    "x.assign_add(tf.constant([3, 3]))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, shapes must be compatible!\n",
    "x.assign(tf.constant([5, 9, -17]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cool (and important) feature: automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(2, name=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppose we want to compute the derivative of x ** 3. Clearly it's 3 x ** 2\n",
    "### We can do it using tf.GradientTape to keep track of the operations on tensor and then compute the gradient afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: to watch a tensor it must be floating point, so we'll cast x\n",
    "x = tf.cast(x, dtype=tf.float16)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x) # we tell the tape to watch variable 'x'\n",
    "    # now we can do operations like x ** 3\n",
    "    y = x ** 3\n",
    "    \n",
    "    \n",
    "## What is y?\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What is the gradient of y wrt x?\n",
    "# we want the gradient of y (x**3) with respect to x\n",
    "grad_xcube = tape.gradient(target=y, sources=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grad_xcube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((3 * x**2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: once we get the gradients from the tape, the resources are released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will cause an error\n",
    "grad_xcube2 = tape.gradient(target=y, sources=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we can create a persistent tape if we want. For example (a bit more complicated example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_np = np.array([1, 2, 3, 4, 5])\n",
    "x = tf.Variable(x_np, name=\"x\", dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tape:\n",
    "    # watch_accessed_variables=True allows us to not have to set each variable we want to watch\n",
    "    \n",
    "    z = tf.constant(7, dtype=tf.float32)\n",
    "    #z = tf.Variable([7, 7, 7, 7, 7], dtype=tf.float32, name='z')\n",
    "    \n",
    "    y = z * tf.math.log(x)\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_y_wrt_x = tape.gradient(target=y, sources=x)\n",
    "print(grad_y_wrt_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_y_wrt_x2 = tape.gradient(target=y, sources=x) # we can grab it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can even grab the gradient with respect to something else (e.g., z)\n",
    "grad_y_wrt_z = tape.gradient(target=y, sources=z)\n",
    "print(grad_y_wrt_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So this is nice but what can we do with it? Let's train linear regression model with Tensorflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this, we'll create some simple data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make up a model\n",
    "true_theta = tf.constant([-1, 5, 2, -7, 3], dtype=tf.float32)[:, tf.newaxis]\n",
    "true_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1500\n",
    "ntr = 1000\n",
    "\n",
    "# make some random data\n",
    "x = tf.constant(tf.random.uniform((n, 5), minval=-1, maxval=+1), dtype=tf.float32)\n",
    "\n",
    "# now calculate the y based on the true parameters\n",
    "y = tf.constant(x @ true_theta, dtype=tf.float32)\n",
    "\n",
    "# split the data\n",
    "train_x = x[:ntr,:]\n",
    "train_y = y[:ntr]\n",
    "\n",
    "val_x = x[ntr:,:].numpy()\n",
    "val_y = y[ntr:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is batch gradient descent\n",
    "def train_lr_tf(x, y, eta=0.05, num_iter=250, verbose=False):\n",
    "    \n",
    "    n, m = x.shape\n",
    "    \n",
    "    # weights / parameters (randomly initialized)\n",
    "    theta = tf.Variable(tf.random.uniform((m, 1), minval=-1, maxval=1), dtype=tf.float32)\n",
    "        \n",
    "    for i in range(0, num_iter):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = tf.linalg.matmul(x, theta) # prediction\n",
    "            mse = tf.reduce_mean(tf.square(y - y_pred)) \n",
    "        \n",
    "        # extract the gradients \n",
    "        gradient_vec = tape.gradient(mse, theta)\n",
    "\n",
    "        # do a gradient descent step (we use assign_sub() to update theta in place)\n",
    "        theta.assign_sub(tf.constant([eta], dtype=tf.float32) * gradient_vec) \n",
    "\n",
    "\n",
    "        if verbose and i % int(num_iter/10) == 0:\n",
    "            print('Iteration {}: the (training) loss (MSE) is {:.5f}'.format(i, mse))\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the training\n",
    "theta = train_lr_tf(x, y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given model parameters 'theta' and a feature matrix 'x', this will return predictions\n",
    "def predict_theta(theta, x):\n",
    "    return np.dot(x, theta) # note: there is no bias 'b' in this case\n",
    "    \n",
    "from sklearn.metrics import r2_score, mean_squared_error, median_absolute_error\n",
    "\n",
    "def print_scores(desc, true_y, pred_y):\n",
    "    r2 = r2_score(true_y, pred_y)\n",
    "    rmse = mean_squared_error(true_y, pred_y, squared=False)\n",
    "    medae = median_absolute_error(true_y, pred_y)\n",
    "    \n",
    "    print('[{}] R^2: {:.2f}, RMSE: {:.2f}, MedAE: {:.2f}'.format(desc, r2, rmse, medae))\n",
    "        \n",
    "print_scores('TF-GD Train', train_y, predict_theta(theta.numpy(), train_x))\n",
    "print_scores('TF-GD Val', val_y, predict_theta(theta.numpy(), val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is nice but it seems tedious. Do we have to implement the gradient descent ourselves and do all the low-level stuff?\n",
    "### => No, we can use a higher-level API like Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to define the architecture\n",
    "def create_model(input_shape, num_outputs=1):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # declare input layer (keras needs to know the number of input features to expect)\n",
    "    model.add(keras.Input(shape=(input_shape[1],))) \n",
    "    \n",
    "    # next add our output layer (1 output, linear activation function)\n",
    "    model.add(keras.layers.Dense(num_outputs, activation='linear'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we create the model (i.e., define the architecture)\n",
    "model = create_model(train_x.shape)\n",
    "\n",
    "# Tip: before you go on, use summary() to check that the architecture is what you intended\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we compile it to specify optimizer, loss, and metrics\n",
    "model.compile(optimizer='sgd', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we train the model\n",
    "model.fit(train_x, train_y, epochs=100, batch_size=50, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can we extract the parameters?\n",
    "def extract_weights(model):\n",
    "    for layer in model.layers:\n",
    "        return layer.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the weights? Are they similar as before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = extract_weights(model)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try a more complex problem with a more complex neural network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll use the Adult data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this case, we'll directly load the Adult dataset pre-processed in a similar way as for assignment 1\n",
    "### and we'll immediately split it into train, test, validation.\n",
    "\n",
    "data_fp = '../data/adult.preproc.npz'\n",
    "data = np.load(data_fp)\n",
    "\n",
    "train_x = data['train_x']; train_y = data['train_y']\n",
    "test_x = data['test_x']; test_y = data['test_y']\n",
    "val_x = data['val_x']; val_y = data['val_y']\n",
    "features = data['features']; labels = data['labels']\n",
    "\n",
    "\n",
    "# check that we have what we expect\n",
    "print('Training: {}, {}'.format(train_x.shape, train_y.shape))\n",
    "print('Test: {}, {}'.format(test_x.shape, test_y.shape))\n",
    "print('Validation: {}, {}'.format(val_x.shape, val_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's train a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the function to define the architecture\n",
    "def create_model_adult(input_shape, hidden_widths=[96, 32], num_outputs=1):\n",
    "    \n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # declare input layer (keras needs to know the number of input features to expect)\n",
    "    model.add(keras.Input(shape=(input_shape[1],))) \n",
    "    \n",
    "    # add two hidden layers with ReLU activation\n",
    "    model.add(keras.layers.Dense(hidden_widths[0], activation='relu'))\n",
    "    model.add(keras.layers.Dense(hidden_widths[1], activation='relu'))\n",
    "    \n",
    "    # next add our output layer (binary classification with 1 output, so sigmoid makes the most sense)\n",
    "    model.add(keras.layers.Dense(num_outputs, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model (i.e., define the architecture)\n",
    "model = create_model_adult(train_x.shape)\n",
    "\n",
    "# Tip: before you go on, use summary() to check that the architecture is what you intended\n",
    "model.summary()\n",
    "\n",
    "# then we compile it to specify optimizer, loss, and metrics\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train the model\n",
    "model.fit(x=train_x, y=train_y, epochs=100, batch_size=100, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x=test_x, y=test_y, verbose=0)\n",
    "print('Test accuracy: {:.2f}%'.format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_adult(train_x.shape)\n",
    "#model.summary()\n",
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# set up tensorboard log directory and callback\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "model.fit(x=train_x, y=train_y, epochs=100, batch_size=100, validation_data=(val_x, val_y), \n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard (notebook experience)\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
