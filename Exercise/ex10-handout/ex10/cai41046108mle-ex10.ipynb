{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10: RNNs & More on Training Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# we'll use keras for neural networks\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# import layers we will use\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, SimpleRNN, GRU\n",
    "\n",
    "# import callbacks we will use\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Let's check our software versions\n",
    "print('### Python version: ' + sys.version)\n",
    "print('### Numpy version: ' + np.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('### Tensorflow version: ' + tf.__version__)\n",
    "print('------------')\n",
    "\n",
    "\n",
    "# load our packages / code\n",
    "sys.path.insert(1, '../common/')\n",
    "import utils\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "\n",
    "seed = 42 # deterministic seed\n",
    "np.random.seed(seed) \n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "prop_vec = [24, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess_fashion_mnist(minmax_normalize=True):\n",
    "    \n",
    "    labels = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    train, testval = fashion_mnist.load_data()\n",
    "    \n",
    "    train_x, train_y = train\n",
    "    testval_x, testval_y = testval\n",
    "    \n",
    "    if minmax_normalize:\n",
    "        train_x = train_x / 255.0\n",
    "        testval_x = testval_x / 255.0\n",
    "    \n",
    "    # split test - val\n",
    "    nval = testval_x.shape[0] // 2\n",
    "    \n",
    "    val_x = testval_x[:nval]\n",
    "    val_y = testval_y[:nval]\n",
    "    \n",
    "    test_x = testval_x[nval:]\n",
    "    test_y = testval_y[nval:]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_fashion_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 25\n",
    "label_idx = train_y[:num_images].astype(int)\n",
    "titles = labels[label_idx]\n",
    "plots.plot_images(train_x[:num_images].reshape(-1, 28, 28), dim_x=28, dim_y=28, fig_size=(9,9), titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subclass for l4 regularization\n",
    "class L4Regularizer(keras.regularizers.Regularizer):\n",
    "    def __init__(self, lmbda):\n",
    "        self.lmbda = lmbda\n",
    "\n",
    "    # for regularizers, constraints, etc. need to define __call__()\n",
    "    def __call__(self, weights):\n",
    "        return self.lmbda * tf.reduce_sum(tf.pow(weights, 4.0))\n",
    "    \n",
    "    # this for config so we can save/load\n",
    "    def get_config(self):\n",
    "        return {'lmbda': self.lmbda}\n",
    "    \n",
    "    \n",
    "# force the weights to be binary (+1 or -1)\n",
    "def binary_weights_constraint(weights):\n",
    "    return tf.where(weights >= 0.0, tf.ones_like(weights), -tf.ones_like(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_functional(input_shape=(28, 28)):  \n",
    "    \n",
    "    # let's use the functional API to create a model\n",
    "    input_layer = Input(shape=input_shape, name='Input')\n",
    "    \n",
    "    flatten_layer = Flatten(name='Flatten')(input_layer)\n",
    "    fc1 = Dense(300, name='FC1', activation='relu', kernel_regularizer=L4Regularizer(0.01), kernel_constraint=binary_weights_constraint)(flatten_layer)\n",
    "    fc2 = Dense(100, name='FC2', activation='relu', kernel_regularizer=L4Regularizer(0.01), kernel_constraint=binary_weights_constraint)(fc1)\n",
    "    output_layer = Dense(10, name='Output', activation='softmax')(fc2)\n",
    "    \n",
    "    model = keras.Model(name='FC-model', inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_functional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the model look like?\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_epochs = 30\n",
    "batch_size = 64\n",
    "history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=max_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how good is our model?\n",
    "loss, acc = model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the weights\n",
    "weights, biases = model.get_layer('FC1').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "biases, biases.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use a RNN to predict stock prices?\n",
    "#### Note: this data is synthetic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '../data/stock-data.csv'\n",
    "stock_data = np.loadtxt(fp, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into features and target\n",
    "all_x = stock_data[:,:-1]\n",
    "all_y = stock_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x, all_y, prop_vec, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7))\n",
    "plt.plot(np.arange(0, train_x.shape[1]), train_x[0])\n",
    "plt.xlabel('Time (day)')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's reshape the data if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_x.shape) < 3:\n",
    "    train_x = train_x[:,:,np.newaxis]\n",
    "    val_x = val_x[:,:,np.newaxis]\n",
    "    test_x = test_x[:,:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_rnn(input_shape=(None,1), verbose=True):\n",
    "    name = 'Simple-RNN'\n",
    "\n",
    "    model = keras.models.Sequential(name=name)\n",
    "\n",
    "    model.add(keras.Input(shape=input_shape, name='input')) \n",
    "    \n",
    "    model.add(SimpleRNN(32, return_sequences=True, name='rnn1'))\n",
    "    model.add(SimpleRNN(32, name='rnn2'))\n",
    "    \n",
    "    # output\n",
    "    model.add(Dense(1, activation='linear', name='output'))\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "        \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=opt, metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_compile_rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "batch_size = 100\n",
    "\n",
    "hist = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=num_epochs, batch_size=batch_size, callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(val_x, verbose=0).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,11))\n",
    "plt.scatter(val_y, val_preds)\n",
    "plt.xlabel('True Price (USD)')\n",
    "plt.ylabel('Predicted Price (USD)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Character-level RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this we'll use the text of Wizard of Oz books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_seq_target(seq_array, window_size, slide=1):\n",
    "    seq_length = seq_array.shape[0]\n",
    "    num_examples_slide1 = seq_length - window_size\n",
    "    x = np.zeros((num_examples_slide1, window_size), dtype=np.uint8)\n",
    "    y = np.zeros((num_examples_slide1,1), dtype=np.uint8)\n",
    "    idx = 0\n",
    "    for i in range(0, num_examples_slide1, slide):\n",
    "        x[idx,:] = seq_array[i:i+window_size]\n",
    "        y[idx] = seq_array[i+window_size]\n",
    "        idx += 1\n",
    "\n",
    "    return x[:idx], y[:idx]\n",
    "\n",
    "def to_array(tokenizer, input_string_array, verbose=0):\n",
    "    # encode as an sequence (array) of integers\n",
    "    seq_list = tokenizer.texts_to_sequences(input_string_array)\n",
    "    # remap to 0 to max_id -1\n",
    "    encoded_array = np.array(seq_list[0], dtype=np.uint8) - 1 # subtract 1 because indices start at 1\n",
    "    if verbose:\n",
    "        print(encoded_array, encoded_array.shape, np.amin(encoded_array), np.amax(encoded_array))\n",
    "    return encoded_array\n",
    "\n",
    "def to_str(tokenizer, array):\n",
    "     return tokenizer.sequences_to_texts(array + 1) # add 1 because indices start at 1\n",
    "\n",
    "def load_preprocess_data(fp = '../data/oz-data.txt', window_size=150, verbose=0):\n",
    "    with open(fp) as f:\n",
    "        input_text = f.read()\n",
    "\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(char_level=True, lower=False)\n",
    "    tokenizer.fit_on_texts(input_text)\n",
    "\n",
    "    num_classes = len(tokenizer.word_index)\n",
    "    \n",
    "    # encode as an sequence (array) of integers\n",
    "    seq_array = to_array(tokenizer, [input_text], verbose)\n",
    "    \n",
    "    # split into windows\n",
    "    x, y = split_data_seq_target(seq_array, window_size, slide=1)\n",
    "    \n",
    "    return x, y, int(num_classes), tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to split this data into train, val, test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What could go wrong if we split randomly (e.g., shuffle x & y, then split)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_seq(x, y, prop_vec=prop_vec, verbose=0):\n",
    "    # instead we take the data in order\n",
    "    n_tr = int(prop_vec[0] / np.sum(prop_vec) * x.shape[0])\n",
    "    n_val = int(prop_vec[1] / np.sum(prop_vec) * x.shape[0])\n",
    "    train_x = x[:n_tr]\n",
    "    train_y = y[:n_tr]\n",
    "    val_x = x[n_tr:n_tr+n_val]\n",
    "    val_y = y[n_tr:n_tr+n_val]\n",
    "    test_x = x[n_tr+n_val:]\n",
    "    test_y = y[n_tr+n_val:]\n",
    "\n",
    "    if verbose:\n",
    "        print(train_x.shape, train_y.shape, val_x.shape, val_y.shape, test_x.shape, test_y.shape)\n",
    "        \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, num_classes, tokenizer = load_preprocess_data()\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = train_test_split_seq(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to one-hot encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds_and_onehot(x, y, num_classes, batch_size=100, prefetch_size=10):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(np.c_[x, y])\n",
    "    ds = ds.map(lambda batch_xy: (batch_xy[:-1], batch_xy[-1]))\n",
    "    ds = ds.map(lambda batch_x, batch_y: (tf.one_hot(batch_x, depth=num_classes), batch_y))   \n",
    "    \n",
    "    # shuffle, batch, and prefetch\n",
    "    ds = ds.shuffle(4096).batch(batch_size)\n",
    "    ds = ds.prefetch(prefetch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = make_ds_and_onehot(train_x, train_y, num_classes)\n",
    "ds_test = make_ds_and_onehot(test_x, test_y, num_classes)\n",
    "ds_val = make_ds_and_onehot(val_x, val_y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in ds_train.take(2):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_rnn(input_shape=(None, num_classes), dropout_rate=0.175, verbose=True):\n",
    "    name = 'CharLevel-RNN'\n",
    "\n",
    "    model = keras.models.Sequential(name=name)\n",
    "\n",
    "    model.add(keras.Input(shape=input_shape, sparse=False, name='input')) \n",
    "    \n",
    "    model.add(GRU(192, return_sequences=True, dropout=dropout_rate, recurrent_dropout=0.0, name='gru1'))\n",
    "    model.add(GRU(128, recurrent_dropout=0.0, name='gru2'))\n",
    "    \n",
    "    # output\n",
    "    model.add(Dense(num_classes, activation='softmax', name='output'))\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "        \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp = './charlevel-rnn.h5'\n",
    "\n",
    "train = False\n",
    "#train = True\n",
    "\n",
    "if train:\n",
    "    model = create_compile_rnn()\n",
    "    \n",
    "    num_epochs = 20\n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=num_epochs, callbacks=[])\n",
    "    \n",
    "    model.save(model_fp) # save the model\n",
    "else:\n",
    "    assert os.path.exists(model_fp), 'Train the model first!'\n",
    "    \n",
    "    model = keras.models.load_model(model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(prompt):\n",
    "    prompt_array = to_array(tokenizer, prompt).reshape(len(prompt), -1)\n",
    "    return tf.one_hot(prompt_array, depth=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_prompt(['Doroth'])\n",
    "prompt_pred = np.argmax(model.predict(prompt), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_str(tokenizer, prompt_pred.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, prompt_str, out_len=50, temp=1.0):\n",
    "    res = ''\n",
    "    for i in range(0, out_len):\n",
    "        prompt = create_prompt([prompt_str + res])\n",
    "        \n",
    "        # get the logits and compute softmax probabilities\n",
    "        prob_vec = model.predict(prompt, verbose=0).reshape(-1,)\n",
    "        logits_vec = np.log(prob_vec)/temp\n",
    "        sample_probas = np.exp(logits_vec)\n",
    "        sample_probas = sample_probas / np.sum(sample_probas)\n",
    "        \n",
    "        # use numpy to sample index according to sample_probas\n",
    "        choice_idx = np.random.choice(np.arange(0, sample_probas.shape[0]), size=1, p=sample_probas)\n",
    "        \n",
    "        chosen_char = to_str(tokenizer, np.array([choice_idx]))[0]\n",
    "        res += chosen_char\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = 'Dorothy said'\n",
    "out_str = sample_from_model(model, prompt_str, out_len=250, temp=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_str + out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
