{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11: Attention & Transformers with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# we'll use keras for neural networks\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# import layers we will use\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, SimpleRNN, GRU\n",
    "\n",
    "# import callbacks we will use\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Let's check our software versions\n",
    "print('### Python version: ' + sys.version)\n",
    "print('### Numpy version: ' + np.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('### Tensorflow version: ' + tf.__version__)\n",
    "print('------------')\n",
    "\n",
    "\n",
    "# load our packages / code\n",
    "sys.path.insert(1, '../common/')\n",
    "import utils\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "\n",
    "seed = 42 # deterministic seed\n",
    "np.random.seed(seed) \n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "prop_vec = [24, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Character-level RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For this we'll use the text of Wizard of Oz books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_seq_target(seq_array, window_size, slide=1):\n",
    "    seq_length = seq_array.shape[0]\n",
    "    num_examples_slide1 = seq_length - window_size\n",
    "    x = np.zeros((num_examples_slide1, window_size), dtype=np.uint8)\n",
    "    y = np.zeros((num_examples_slide1,1), dtype=np.uint8)\n",
    "    idx = 0\n",
    "    for i in range(0, num_examples_slide1, slide):\n",
    "        x[idx,:] = seq_array[i:i+window_size]\n",
    "        y[idx] = seq_array[i+window_size]\n",
    "        idx += 1\n",
    "\n",
    "    return x[:idx], y[:idx]\n",
    "\n",
    "def to_array(tokenizer, input_string_array, verbose=0):\n",
    "    # encode as an sequence (array) of integers\n",
    "    seq_list = tokenizer.texts_to_sequences(input_string_array)\n",
    "    # remap to 0 to max_id -1\n",
    "\n",
    "    encoded_array = np.array(seq_list[0], dtype=np.uint8) - 1 # subtract 1 because indices start at 1\n",
    "    if verbose:\n",
    "        print(encoded_array, encoded_array.shape, np.amin(encoded_array), np.amax(encoded_array))\n",
    "    return encoded_array\n",
    "\n",
    "def to_str(tokenizer, array):\n",
    "     return tokenizer.sequences_to_texts(array + 1) # add 1 because indices start at 1\n",
    "\n",
    "def load_preprocess_data(fp = '../data/oz-data.txt', window_size=150, verbose=0):\n",
    "    with open(fp) as f:\n",
    "        input_text = f.read()\n",
    "\n",
    "    tokenizer = keras.preprocessing.text.Tokenizer(char_level=True, lower=False)\n",
    "    tokenizer.fit_on_texts(input_text)\n",
    "\n",
    "    num_classes = len(tokenizer.word_index)\n",
    "    \n",
    "    # encode as an sequence (array) of integers\n",
    "    seq_array = to_array(tokenizer, [input_text], verbose)\n",
    "    \n",
    "    # split into windows\n",
    "    x, y = split_data_seq_target(seq_array, window_size, slide=1)\n",
    "    \n",
    "    return x, y, int(num_classes), tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We want to split this data into train, val, test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What could go wrong if we split randomly (e.g., shuffle x & y, then split)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_seq(x, y, prop_vec=prop_vec, verbose=0):\n",
    "    # instead we take the data in order\n",
    "    n_tr = int(prop_vec[0] / np.sum(prop_vec) * x.shape[0])\n",
    "    n_val = int(prop_vec[1] / np.sum(prop_vec) * x.shape[0])\n",
    "    train_x = x[:n_tr]\n",
    "    train_y = y[:n_tr]\n",
    "    val_x = x[n_tr:n_tr+n_val]\n",
    "    val_y = y[n_tr:n_tr+n_val]\n",
    "    test_x = x[n_tr+n_val:]\n",
    "    test_y = y[n_tr+n_val:]\n",
    "\n",
    "    if verbose:\n",
    "        print(train_x.shape, train_y.shape, val_x.shape, val_y.shape, test_x.shape, test_y.shape)\n",
    "        \n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 150\n",
    "x, y, num_classes, tokenizer = load_preprocess_data(window_size=window_size)\n",
    "train_x, train_y, val_x, val_y, test_x, test_y = train_test_split_seq(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to one-hot encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds_and_onehot(x, y, num_classes, batch_size=100, prefetch_size=10):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(np.c_[x, y])\n",
    "    ds = ds.map(lambda batch_xy: (batch_xy[:-1], batch_xy[-1]))\n",
    "    ds = ds.map(lambda batch_x, batch_y: (tf.one_hot(batch_x, depth=num_classes), batch_y))   \n",
    "    \n",
    "    # shuffle, batch, and prefetch\n",
    "    ds = ds.shuffle(4096).batch(batch_size)\n",
    "    ds = ds.prefetch(prefetch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = make_ds_and_onehot(train_x, train_y, num_classes)\n",
    "ds_test = make_ds_and_onehot(test_x, test_y, num_classes)\n",
    "ds_val = make_ds_and_onehot(val_x, val_y, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in ds_train.take(2):\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_rnn(input_shape=(None, num_classes), dropout_rate=0.175, verbose=True):\n",
    "    name = 'CharLevel-RNN'\n",
    "\n",
    "    model = keras.models.Sequential(name=name)\n",
    "\n",
    "    model.add(keras.Input(shape=input_shape, sparse=False, name='input')) \n",
    "    \n",
    "    model.add(GRU(192, return_sequences=True, dropout=dropout_rate, recurrent_dropout=0.0, name='gru1'))\n",
    "    model.add(GRU(128, recurrent_dropout=0.0, name='gru2'))\n",
    "    \n",
    "    # output\n",
    "    model.add(Dense(num_classes, activation='softmax', name='output'))\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "        \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp = './charlevel-rnn.h5'\n",
    "\n",
    "train = False\n",
    "#train = True\n",
    "\n",
    "if train:\n",
    "    model = create_compile_rnn()\n",
    "    \n",
    "    num_epochs = 20\n",
    "    history = model.fit(ds_train, validation_data=ds_val, epochs=num_epochs, callbacks=[])\n",
    "    \n",
    "    model.save(model_fp) # save the model\n",
    "else:\n",
    "    assert os.path.exists(model_fp), 'Train the model first!'\n",
    "    \n",
    "    model = keras.models.load_model(model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(prompt):\n",
    "    prompt_array = to_array(tokenizer, prompt).reshape(len(prompt), -1)\n",
    "    return tf.one_hot(prompt_array, depth=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = create_prompt(['Doroth'])\n",
    "prompt_pred = np.argmax(model.predict(prompt, verbose=0), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_str(tokenizer, prompt_pred.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(prompt, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(model, prompt_str, out_len=50, temp=1.0):\n",
    "    res = ''\n",
    "    for i in range(0, out_len):\n",
    "        prompt = create_prompt([prompt_str + res])\n",
    "        \n",
    "        # get the logits and compute softmax probabilities\n",
    "        prob_vec = model.predict(prompt, verbose=0).reshape(-1,)\n",
    "        logits_vec = np.log(prob_vec)/temp\n",
    "        sample_probas = np.exp(logits_vec)\n",
    "        sample_probas = sample_probas / np.sum(sample_probas)\n",
    "        \n",
    "        # use numpy to sample index according to sample_probas\n",
    "        choice_idx = np.random.choice(np.arange(0, sample_probas.shape[0]), size=1, p=sample_probas)\n",
    "        \n",
    "        chosen_char = to_str(tokenizer, np.array([choice_idx]))[0]\n",
    "        res += chosen_char\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_str = 'Dorothy said'\n",
    "out_str = sample_from_model(model, prompt_str, out_len=250, temp=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_str + out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we train a character-level prediction model using the Transformer architecture??\n",
    "### Transformer paper: https://arxiv.org/pdf/1706.03762.pdf\n",
    "# ![Transformer](https://upload.wikimedia.org/wikipedia/commons/8/8f/The-Transformer-model-architecture.png)\n",
    "### (Image source: wikipedia.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" let's create a custom Keras layer to implement a transformer layer/block.\n",
    "\"\"\"\n",
    "from keras.layers import MultiHeadAttention, LayerNormalization, Dense, Dropout\n",
    "\n",
    "class TransformerWithMHALayer(keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_heads, dense_units, embedding_size,  \n",
    "                 attention_dropout=0.0, dense_dropout=0.1, dense_activation='relu'):\n",
    "        super().__init__() # super init\n",
    "\n",
    "        # let's instantiate the layers/components and store them.\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_size, dropout=attention_dropout) \n",
    "\n",
    "        # we need two of these one for each add-norm\n",
    "        self.layernorm1 = LayerNormalization()\n",
    "        self.layernorm2 = LayerNormalization()\n",
    "\n",
    "        # need a dropout layer after dense\n",
    "        self.dropout = Dropout(dense_dropout)\n",
    "\n",
    "        # feedforward part - dense followed by dense \n",
    "        self.dense1 = Dense(dense_units, activation=dense_activation)\n",
    "        self.dense2 = Dense(embedding_size, activation='linear') # embedding_size output (so shapes work and we can stack transformer layers)\n",
    "        \n",
    "        self.dropout2 = Dropout(dense_dropout)\n",
    "\n",
    "\n",
    "    \"\"\" This is invoked during the forward pass. It needs to implement the forward pass functionality of a MHA transformer layer.\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        # what we need to do. 1. MHA, 2. layernorm with residual connection (add-norm), 3. feedforward (dense, dense), 4. dropout, 5. add-norm\n",
    "        attention_output = self.mha(inputs, inputs) # 1. MHA\n",
    "        \n",
    "        first_addnorm = self.layernorm1(inputs + attention_output) # 2. note the residual connection ('inputs')\n",
    "\n",
    "        ff1 = self.dense1(first_addnorm) # 3. feedforward\n",
    "        ff2 = self.dense2(ff1)\n",
    "        ffdropout = self.dropout(ff2)   # 4. dropout\n",
    "\n",
    "        second_addnorm = self.layernorm2(first_addnorm + ffdropout) # 5. again note the residual connection ('first_addnorm')\n",
    "\n",
    "        return second_addnorm # this is the output of the layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras has a bunch of examples/tutorials for doing various things with transformers. For example see: https://keras.io/examples/nlp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement the create compile for our transformer.\n",
    "\n",
    "from keras.layers import Input, GlobalMaxPool1D, Embedding\n",
    "\n",
    "def create_compile_transformer(max_prompt_len, num_classes, embedding_size=96, num_attention_heads=6, \n",
    "                                     dense_units=48, verbose=True):\n",
    "    name = 'CharLevel-Transformer'\n",
    "\n",
    "    inputs = Input(shape=(max_prompt_len))\n",
    "\n",
    "    # This is just a simple (char) embedding, ideally we should do some kind of positional embedding instead\n",
    "    # TODO: replace this embedding with (char and) positional embedding\n",
    "    # e.g., see: https://keras.io/api/keras_nlp/modeling_layers/token_and_position_embedding/\n",
    "    # and https://keras.io/api/keras_nlp/modeling_layers/position_embedding/\n",
    "    embeddings = Embedding(num_classes, embedding_size)(inputs)\n",
    "    \n",
    "    transformer1 = TransformerWithMHALayer(num_attention_heads, dense_units, embedding_size)(embeddings)\n",
    "    \n",
    "    # we could stack additional transformer layers for example:\n",
    "    # transformer2 = TransformerWithMHALayer(embedding_size, num_attention_heads, ff_dense_units)(transformer1)\n",
    "\n",
    "    maxpool = GlobalMaxPool1D()(transformer1)\n",
    "    dropout1 = Dropout(0.125)(maxpool)\n",
    "    dense2 = Dense(90, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.125)(dense2)\n",
    "    outputs = Dense(num_classes, activation='softmax')(dropout2)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=name)\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the model to see the summary\n",
    "_ = create_compile_transformer(window_size, num_classes, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (At home) exercise: complete the notebook\n",
    "### [TODO] 1. Add positional embedding (note: it may work somewhat without it if window size is small enough)\n",
    "### [TODO] 2. Add some training code\n",
    "### [TODO] 3. To sample you will need to change the way to get predictions from the model and sample from it\n",
    "### In particular note that we are not using one-hot encoding over the tokens (char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
