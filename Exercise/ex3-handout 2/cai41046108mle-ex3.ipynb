{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Wine Regression?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Let's check our software versions\n",
    "print('### Python version: ' + __import__('sys').version)\n",
    "print('### NumPy version: ' + np.__version__)\n",
    "print('### SciPy version: ' + sp.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('------------')\n",
    "\n",
    "# load our packages / code\n",
    "sys.path.insert(1, '../common/')\n",
    "import utils\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "seed = 17\n",
    "\n",
    "np.random.seed(seed) # deterministic seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting stuff starts now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to load the data from compressed CSV\n",
    "wine_type = 'red'\n",
    "#wine_type = 'white'\n",
    "\n",
    "df = pd.read_csv('../data/{}-wine-quality.csv'.format(wine_type), header=0, na_values='?', sep=' *; *', skipinitialspace=True, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we loaded the data as expected\n",
    "if wine_type == 'white':\n",
    "    df_expected_shape = (4898,12)\n",
    "else:\n",
    "    df_expected_shape = (1599,12)\n",
    "    \n",
    "assert df.shape == df_expected_shape, 'Unexpected shape of df!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick tip: use info() to get a glance at the size and attributes of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a few rows of our dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many records do we have?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## header right now: fixed acidity;volatile acidity;citric acid;residual sugar;chlorides;free sulfur dioxide;total sulfur dioxide;density;pH;sulphates;alcohol;quality\n",
    "col_names = df.columns\n",
    "col_names = [x for x in col_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all columns are numerical and the last one 'quality' is what we want to predict\n",
    "#### Note: quality is a score between 0 (very bad) and 10 (excellent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the data as a numpy array\n",
    "all_xy = np.asarray(df, dtype='float64')\n",
    "assert all_xy.shape[1] == 12\n",
    "\n",
    "label_col_idx = all_xy.shape[1]-1\n",
    "features_col_idx = range(0, label_col_idx)\n",
    "\n",
    "feature_names = col_names[0:label_col_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's separate features from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from the label\n",
    "all_x = all_xy[:,features_col_idx]\n",
    "all_y = all_xy[:,label_col_idx]\n",
    "all_y = all_y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test, Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split between train, test, and validation\n",
    "prop_vec = [14, 3, 3]\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats & Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the distribution of labels look like?\n",
    "label_name = col_names[-1]\n",
    "utils.print_array_hist(train_y, label=label_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clearly, this is not a balanced dataset (we will see later on why this can matter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot a histogram to visualize the distribution of labels\n",
    "bins = np.arange(-1, 11) + 0.5\n",
    "\n",
    "plt.hist(train_y, bins, density=False, alpha=0.5, edgecolor='k', label=label_name)\n",
    "\n",
    "plt.xticks(np.arange(11))\n",
    "plt.xlabel(label_name)\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: what do you think is a good baseline for predicting the quality exactly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the distribution of features look like?\n",
    "for i in range(train_x.shape[1]):\n",
    "    utils.print_array_basic_stats(train_x[:, i], label=col_names[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Do the features even help us predict the quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature distribution based on quality\n",
    "\n",
    "#feature_idx = 0; bins = np.linspace(3, 12, 12)\n",
    "#feature_idx = 3; bins = np.linspace(0, 70, 20)\n",
    "feature_idx = 10; bins = np.linspace(7, 15, 12)\n",
    "\n",
    "lowq_idx = train_y == 4 # low quality wines\n",
    "highq_idx = train_y == 8 # high quality wines\n",
    "\n",
    "plt.hist(train_x[lowq_idx,feature_idx], bins, density=True, alpha=0.5, edgecolor='k', label='Low quality')\n",
    "plt.hist(train_x[highq_idx,feature_idx], bins, density=True, alpha=0.5, edgecolor='k', label='High quality')\n",
    "\n",
    "plt.xlabel('{}'.format(col_names[feature_idx]))\n",
    "plt.ylabel('Density')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we look at the statistical information that features contain about the task in a systematic way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: this may be in your assignment!\n",
    "\n",
    "train_xy = np.hstack((train_x, train_y.reshape(-1, 1)))\n",
    "\n",
    "pairwise_corr = np.corrcoef(train_xy, rowvar=False)\n",
    "\n",
    "plots.heatmap(pairwise_corr, col_names, col_names, rot=90, fsz=(14, 14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Left as exercise]: use Pandas' scatter_matrix to look at scatter plots for the correlation. *Good exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ref: https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(copy=True)\n",
    "scaler.fit(all_x) \n",
    "\n",
    "scaled_all_x = scaler.transform(all_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's split the *scaled* data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split between train, test, and validation!\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the distribution of features look like now that we have done scaling?\n",
    "for i in range(train_x.shape[1]):\n",
    "    utils.print_array_basic_stats(train_x[:, i], label=col_names[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use the normal equation\n",
    "X = train_x\n",
    "y = train_y\n",
    "\n",
    "# add a constant feature of 1 to each row to account for the bias term\n",
    "X_with_b = np.c_[np.ones((X.shape[0],1)), X]\n",
    "\n",
    "# theta = (X^T X)^(-1) X^T  y\n",
    "XTX = X_with_b.T @ X_with_b\n",
    "# note this is exactly the same as: XTX = np.matmul(X_with_b.T, X_with_b)\n",
    "# and also exactly the same as: XTX = np.dot(X_with_b.T, X_with_b)\n",
    "\n",
    "theta = np.linalg.inv(XTX) @ X_with_b.T @ y\n",
    "\n",
    "b = theta[0]\n",
    "w = theta[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.4f}'.format})\n",
    "print('Trained model -- w: {}, b: {}'.format(w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_corr_quality = pairwise_corr[-1,0:-1]\n",
    "print(feature_corr_quality)\n",
    "\n",
    "corr = np.corrcoef(feature_corr_quality, w, rowvar=False)\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg_predict(x):\n",
    "    return np.dot(x, w) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = linreg_predict(train_x)\n",
    "\n",
    "print(y_pred[0:10], train_y[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_predict(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "train_acc = accuracy_score(y_pred.astype(int), train_y)\n",
    "print(train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good is our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_pred = linreg_predict(train_x)\n",
    "val_pred = linreg_predict(val_x)\n",
    "\n",
    "# measure the error (MSE) wrt true quality score\n",
    "train_error = mean_squared_error(train_pred, train_y)\n",
    "val_error = mean_squared_error(val_pred, val_y)\n",
    "\n",
    "print('Training error (MSE): {:.3f}, Validation error (MSE): {:.3f}'.format(train_error, val_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about MAE?\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# measure the error (MAE) wrt true quality score\n",
    "train_error = mean_absolute_error(train_pred, train_y)\n",
    "val_error = mean_absolute_error(val_pred, val_y)\n",
    "\n",
    "print('Training error (MAE): {:.3f}, Validation error (MAE): {:.3f}'.format(train_error, val_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is 0.45 MSE a good model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need a baseline to compare this to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple baseline: always predict the mode (most frequent value)\n",
    "mode = stats.mode(train_y, keepdims=False)[0]\n",
    "baseline_pred = np.ones_like(train_pred) * mode\n",
    "print('Baseline prediction mode: {}'.format(mode))\n",
    "\n",
    "baseline_error = mean_squared_error(baseline_pred, train_y)\n",
    "print('Baseline error (MSE): {:.3f}'.format(baseline_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train a linear regression model but this time with scikit-learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import what we need from scikit-learn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "## ref: https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "\n",
    "lrmodel = LinearRegression().fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to evaluate the model is R^2 the coefficient of determination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is R^2? It's defined as: $ R^2 = 1 - \\frac{s_{\\rm err}}{s_{\\rm tot}} $ where: \n",
    "### $ s_{\\rm err} = \\sum_i (y_i - f_{{\\bf w},b}(x_i))^2 \\quad $   (called \"residual sum of squares\" --- the squared error sum for the data)\n",
    "### $ s_{\\rm tot} = \\sum_i (y_i - \\bar{y})^2 \\quad $     (called \"total sum of squares\" --- the variance of the data)\n",
    "### Here $f_{w,b}(x_i)$ is the predicted value on example $i$ and $\\bar{y}$ is average $y$ value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions: \n",
    "### 1. What is $R^2$ if we always predict the average $\\bar{y}$?\n",
    "### 2. What is the maximum value for $R^2$?\n",
    "### 3. What does it mean if $R^2 < 0$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2 the coefficient of determination\n",
    "r2_train = lrmodel.score(train_x, train_y)\n",
    "r2_val = lrmodel.score(val_x, val_y)\n",
    "\n",
    "print('Train R^2: {:.3f}, Val  R^2: {:.3f}'.format(r2_train, r2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.6f}'.format})\n",
    "print('Normal Equation LinearRegression model -- w: {}, b: {}'.format(w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlr = lrmodel.coef_\n",
    "blr = lrmodel.intercept_\n",
    "print('Scikit-learn LinearRegression model -- w: {}, b: {}'.format(wlr, blr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The two models are identical!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "# Let's use SGDRegressor: https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.SGDRegressor.html\n",
    "\n",
    "sgdlrmodel = SGDRegressor(loss=\"squared_error\", penalty=None, random_state=seed).fit(train_x, train_y)\n",
    "\n",
    "wsgd = sgdlrmodel.coef_\n",
    "bsgd = sgdlrmodel.intercept_\n",
    "print('SGDRegressor model -- w: {}, b: {}'.format(wsgd, bsgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This time the model is a little bit different!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_mse(model):\n",
    "    # use the model to predict the quality on training data and validation data\n",
    "    train_pred = model.predict(train_x)\n",
    "    val_pred = model.predict(val_x)\n",
    "\n",
    "    # measure the error (MSE) wrt true quality score\n",
    "    train_error = mean_squared_error(train_pred, train_y)\n",
    "    val_error = mean_squared_error(val_pred, val_y)\n",
    "\n",
    "    print('Training error (MSE): {:.3f}, Validation error (MSE): {:.3f}'.format(train_error, val_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_mse(sgdlrmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we regularize the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import what we need from scikit-learn\n",
    "from sklearn.linear_model import Ridge\n",
    "## ref: https://scikit-learn.org/0.24/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "ridgemodel = Ridge(alpha=1.0).fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ridge LinearRegression model -- w: {}, b: {}'.format(ridgemodel.coef_, ridgemodel.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlr = lrmodel.coef_\n",
    "blr = lrmodel.intercept_\n",
    "print('Scikit-learn LinearRegression model -- w: {}, b: {}'.format(wlr, blr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(ridgemodel.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we train a highly regularized model? What do the weights look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highreg_ridge = Ridge(alpha=10000.0).fit(train_x, train_y)\n",
    "\n",
    "print('Ridge LinearRegression model -- w: {}, b: {}'.format(highreg_ridge.coef_, highreg_ridge.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(highreg_ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the model any good?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_mse(highreg_ridge)\n",
    "\n",
    "# R^2 the coefficient of determination\n",
    "r2_train = highreg_ridge.score(train_x, train_y)\n",
    "r2_val = highreg_ridge.score(val_x, val_y)\n",
    "\n",
    "print('Train R^2: {:.3f}, Val  R^2: {:.3f}'.format(r2_train, r2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play with polynomial regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of adding adding polynomial combinations of features manually, we can use Scikit-learn to do it for us!\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polyf = PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n",
    "all_x_polyf = polyf.fit_transform(scaled_all_x.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scaled_all_x.shape)\n",
    "print(all_x_polyf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split between train, test, and validation!\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x_polyf, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a linear regression model again\n",
    "polyregmodel = LinearRegression().fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_mse(polyregmodel)\n",
    "\n",
    "# R^2 the coefficient of determination\n",
    "r2_train = polyregmodel.score(train_x, train_y)\n",
    "r2_val = polyregmodel.score(val_x, val_y)\n",
    "\n",
    "print('Train R^2: {:.3f}, Val  R^2: {:.3f}'.format(r2_train, r2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Polynomial Regression model -- w: {}, b: {}'.format(polyregmodel.coef_, polyregmodel.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = polyregmodel.predict(train_x)\n",
    "val_pred = polyregmodel.predict(val_x)\n",
    "\n",
    "print(train_pred[0:5], train_y[0:5])\n",
    "print(val_pred[0:5], val_y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_error_idx = np.argmax(np.abs(train_pred - train_y))\n",
    "print('Target: {}, Predicted: {}, Error: {}'.format(\n",
    "    train_y[max_error_idx], train_pred[max_error_idx], train_pred[max_error_idx] - train_y[max_error_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_error_idx = np.argmax(np.abs(val_pred - val_y))\n",
    "print('Target: {}, Predicted: {}, Error: {}'.format(\n",
    "    val_y[max_error_idx], val_pred[max_error_idx], val_pred[max_error_idx] - val_y[max_error_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *[Left as exercise] Train a regularize Polynomial Regression model with degree 3.  Can you regularize enough to avoid overfitting but still benefit from polynomial features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's train a linear regression model again\n",
    "polyregmodel = Ridge(alpha=100.0).fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_mse(polyregmodel)\n",
    "\n",
    "# R^2 the coefficient of determination\n",
    "r2_train = polyregmodel.score(train_x, train_y)\n",
    "r2_val = polyregmodel.score(val_x, val_y)\n",
    "\n",
    "print('Train R^2: {:.3f}, Val  R^2: {:.3f}'.format(r2_train, r2_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's go back to Linear Regression. What is the most important feature? How can we tell?\n",
    "wlr = lrmodel.coef_\n",
    "\n",
    "mwfidx = np.argmax(np.abs(wlr))\n",
    "print('Most important feature: {} (weight: {:.3f})'.format(col_names[mwfidx], wlr[mwfidx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can happen if we have multicollinearity? Let's add a near duplicate feature and see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names2 = feature_names\n",
    "feature_names2.append('alcohol-dup')\n",
    "\n",
    "all_x_alc = all_x[:,mwfidx]\n",
    "\n",
    "sigma = 0.01\n",
    "alcdup = all_x_alc + np.random.randn(all_x.shape[0]) * sigma\n",
    "\n",
    "all_x_dup = np.c_[all_x, alcdup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alcdup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x_dup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(copy=True)\n",
    "scaled_all_x_dup = scaler.fit_transform(all_x_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split between train, test, and validation!\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x_dup, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dup Feature LinearRegression model -- w: {}, b: {}'.format(model.coef_, model.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdup = model.coef_\n",
    "\n",
    "mwfidx = np.argmax(np.abs(wdup))\n",
    "print('Most important feature: {} (weight: {:.3f})'.format(feature_names2[mwfidx], wdup[mwfidx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_mse(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split between train, test, and validation!\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x_dup, all_y, prop_vec, shuffle=True, seed=seed-1)\n",
    "\n",
    "model = LinearRegression().fit(train_x, train_y)\n",
    "print('Dup Feature LinearRegression model -- w: {}, b: {}'.format(model.coef_, model.intercept_))\n",
    "\n",
    "wdup = model.coef_\n",
    "\n",
    "mwfidx = np.argmax(np.abs(wdup))\n",
    "print('\\nMost important feature: {} (weight: {:.3f})'.format(feature_names2[mwfidx], wdup[mwfidx]))\n",
    "\n",
    "evaluate_model_mse(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split between train, test, and validation!\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x_dup, all_y, prop_vec, shuffle=True, seed=seed*2)\n",
    "\n",
    "model = LinearRegression().fit(train_x, train_y)\n",
    "print('Dup Feature LinearRegression model -- w: {}, b: {}'.format(model.coef_, model.intercept_))\n",
    "\n",
    "wdup = model.coef_\n",
    "\n",
    "mwfidx = np.argmax(np.abs(wdup))\n",
    "print('\\nMost important feature: {} (weight: {:.3f})'.format(feature_names2[mwfidx], wdup[mwfidx]))\n",
    "evaluate_model_mse(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
