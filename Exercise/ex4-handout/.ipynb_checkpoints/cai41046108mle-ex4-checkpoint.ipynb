{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Fun with Trees and Ensembles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats as stats\n",
    "import sklearn\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "\n",
    "# Let's check our software versions\n",
    "print('### Python version: ' + __import__('sys').version)\n",
    "print('### NumPy version: ' + np.__version__)\n",
    "print('### SciPy version: ' + sp.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('------------')\n",
    "\n",
    "# load our packages / code\n",
    "sys.path.insert(1, '../common/')\n",
    "import utils\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "seed = 42\n",
    "\n",
    "np.random.seed(seed) # deterministic seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting stuff starts now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models\n",
    "# define a function to calculate accuracy\n",
    "def model_accuracy(model, x, true_y):\n",
    "    pred = model.predict(x)\n",
    "    return np.sum(pred == true_y) / true_y.shape[0]\n",
    "\n",
    "def plot_decision_boundary(model, X, y, fsz=(14,7), xlim=None, ylim=None, \n",
    "                           num_classes = 2, plot_colors = \"rb\", plot_step = 0.02, classes=['Class 1', 'Class 2']):  \n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.figure(figsize=fsz)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # plot the decision regions (& boundary)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.5)\n",
    "\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "\n",
    "    # plot the training points\n",
    "    for i, color in zip(range(num_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=classes[i],\n",
    "                    #cmap=plt.cm.RdBu, \n",
    "                    edgecolor='black', s=80, linewidth=2, alpha=0.75)\n",
    "\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    if xlim is not None:\n",
    "        plt.xlim(xlim)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# let's create a synthetic data so we can explore\n",
    "def create_synth_data():\n",
    "    n = 100\n",
    "    sigma1 = 0.5\n",
    "    sigma2 = 0.4\n",
    "    x1 = np.c_[(np.random.randn(n) - 1)*sigma1, (np.random.randn(n) + 0.5)*sigma1]\n",
    "    x2 = np.c_[(np.random.randn(n) + 1)*sigma2, (np.random.randn(n) -0.6)*sigma2]\n",
    "    x = np.r_[x1, x2]\n",
    "\n",
    "    y = np.zeros((2*n,))\n",
    "    y[n:] = 1\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = create_synth_data()\n",
    "\n",
    "# Decision tree with restricted depth\n",
    "dt = DecisionTreeClassifier(max_depth=3, random_state=seed).fit(x, y)\n",
    "plot_decision_boundary(dt, x, y)\n",
    "\n",
    "\n",
    "dt_train_acc = model_accuracy(dt, x, y)\n",
    "\n",
    "print('[Decision Tree] Training accuracy: {:.2f}%'.format(dt_train_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,14))\n",
    "\n",
    "plot_tree(dt,filled=True, label='all', rounded=True, fontsize=13)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now train a tree with no max_depth restrictions\n",
    "dt = DecisionTreeClassifier(max_depth=None, random_state=seed).fit(x, y)\n",
    "plot_decision_boundary(dt, x, y)\n",
    "\n",
    "dt_train_acc = model_accuracy(dt, x, y)\n",
    "print('[Decision Tree] Training accuracy: {:.2f}%'.format(dt_train_acc*100))\n",
    "\n",
    "test_x, test_y = create_synth_data()\n",
    "\n",
    "dt_test_acc = model_accuracy(dt, test_x, test_y)\n",
    "print('[Decision Tree] Test accuracy: {:.2f}%'.format(dt_test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,20))\n",
    "\n",
    "plot_tree(dt,filled=True, label='all', rounded=True, fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What kind of decision boundaries can our trees produce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1500\n",
    "\n",
    "# uniform random points within -1, 1\n",
    "x = np.c_[(np.random.random(n)-0.5)*2, (np.random.random(n)-0.5)*2]\n",
    "\n",
    "# set class label so points above x2 = x1^2 are class 2 and points below are class 1\n",
    "y = np.zeros((n,))\n",
    "class2_idx = np.where(x[:,0]**2 <= x[:,1])[0]\n",
    "y[class2_idx] = 1\n",
    "\n",
    "# train decision tree\n",
    "dt = DecisionTreeClassifier(max_depth=None, random_state=seed).fit(x, y)\n",
    "plot_decision_boundary(dt, x, y, xlim=[-1.05,1.05], ylim=[-1.05,1.05])\n",
    "\n",
    "dt_train_acc = model_accuracy(dt, x, y)\n",
    "\n",
    "print('[Decision Tree] Training accuracy: {:.2f}%'.format(dt_train_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we train a random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=500, max_depth=None, max_samples=1000, random_state=seed).fit(x, y)\n",
    "\n",
    "plot_decision_boundary(rf, x, y, xlim=[-1.05,1.05], ylim=[-1.05,1.05])\n",
    "\n",
    "rf_train_acc = model_accuracy(dt, x, y)\n",
    "\n",
    "print('[Random Forest] Training accuracy: {:.2f}%'.format(rf_train_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about rotations of the feature space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a synthetic data so we can explore\n",
    "\n",
    "def create_synth_data2():\n",
    "    n = 100\n",
    "    sigma1 = 0.5\n",
    "    sigma2 = 0.5\n",
    "    x1 = np.c_[np.random.randn(n)*2, (np.random.randn(n) - 2.5)*sigma1]\n",
    "    x2 = np.c_[np.random.randn(n)*2, (np.random.randn(n) + 2.5)*sigma2]\n",
    "    x = np.r_[x1, x2]\n",
    "\n",
    "    y = np.zeros((2*n,))\n",
    "    y[n:] = 1\n",
    "    y = y.astype(int)\n",
    "    \n",
    "    return x, y\n",
    "            \n",
    "\n",
    "x, y = create_synth_data2()\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=1, random_state=seed).fit(x, y)\n",
    "plot_decision_boundary(dt, x, y)\n",
    "\n",
    "\n",
    "dt_train_acc = model_accuracy(dt, x, y)\n",
    "\n",
    "print('[Decision Tree] Training accuracy: {:.2f}%'.format(dt_train_acc*100))\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plot_tree(dt,filled=True, label='all', rounded=True, fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's rotate the features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rotation matrix\n",
    "theta = np.radians(45)\n",
    "c = np.cos(theta)\n",
    "s = np.sin(theta)\n",
    "R = np.array(((c, -s), (s, c)))\n",
    "\n",
    "# let's rotate the data!\n",
    "xrot = R.dot(x.T).T\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=None, random_state=seed).fit(xrot, y)\n",
    "plot_decision_boundary(dt, xrot, y)\n",
    "\n",
    "dt_train_acc = model_accuracy(dt, xrot, y)\n",
    "print('[Decision Tree (rot)] Training accuracy: {:.2f}%'.format(dt_train_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "plot_tree(dt,filled=True, label='all', rounded=True, fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "model = LogisticRegression().fit(x, y)\n",
    "acc = model_accuracy(model, x, y)\n",
    "\n",
    "model_rot = LogisticRegression().fit(xrot, y)\n",
    "acc_rot = model_accuracy(model_rot, xrot, y)\n",
    "\n",
    "print('[Logistic Regression] Accuracy (orig): {:.2f}%, Accuracy (rotated): {:.2f}%'.format(acc*100, acc_rot*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(model, x, y)\n",
    "plot_decision_boundary(model_rot, xrot, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BTW, for logistic regression, we have probabilities. Let's see how confident is our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unreg_model = LogisticRegression(C = 100).fit(x, y)\n",
    "reg_model = LogisticRegression(C = 0.01).fit(x, y)\n",
    "\n",
    "y_pred_proba_unreg = unreg_model.predict_proba(x)\n",
    "y_pred_proba_reg = reg_model.predict_proba(x)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "bins = np.linspace(0, 1, 20)\n",
    "plt.hist(np.amax(y_pred_proba_unreg, axis=1), bins, density=False, alpha=0.5, edgecolor='k', label='Low regularization')\n",
    "plt.hist(np.amax(y_pred_proba_reg, axis=1), bins, density=False, alpha=0.5, edgecolor='k', label='High regularization')\n",
    "\n",
    "plt.xlabel('Probability of label')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's load the wine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to load the data from compressed CSV\n",
    "#wine_type = 'red'\n",
    "wine_type = 'white'\n",
    "\n",
    "df = pd.read_csv('../data/{}-wine-quality.csv'.format(wine_type), header=0, na_values='?', sep=' *; *', skipinitialspace=True, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we loaded the data as expected\n",
    "if wine_type == 'white':\n",
    "    df_expected_shape = (4898,12)\n",
    "else:\n",
    "    df_expected_shape = (1599,12)\n",
    "    \n",
    "assert df.shape == df_expected_shape, 'Unexpected shape of df!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick tip: use info() to get a glance at the size and attributes of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at a few rows of our dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many records do we have?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## header right now: fixed acidity;volatile acidity;citric acid;residual sugar;chlorides;free sulfur dioxide;total sulfur dioxide;density;pH;sulphates;alcohol;quality\n",
    "col_names = df.columns\n",
    "col_names = [x for x in col_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### all columns are numerical and the last one 'quality' is what we want to predict\n",
    "#### Note: quality is a score between 0 (very bad) and 10 (excellent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all the data as a numpy array\n",
    "all_xy = np.asarray(df, dtype='float64')\n",
    "assert all_xy.shape[1] == 12\n",
    "\n",
    "label_col_idx = all_xy.shape[1]-1\n",
    "features_col_idx = range(0, label_col_idx)\n",
    "\n",
    "feature_names = col_names[0:label_col_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's separate features from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from the label\n",
    "all_x = all_xy[:,features_col_idx]\n",
    "all_y = all_xy[:,label_col_idx]\n",
    "all_y = all_y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Test, Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split between train, test, and validation\n",
    "prop_vec = [14, 3, 3]\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(all_x, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check shapes\n",
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats & Looking at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the distribution of labels look like?\n",
    "label_name = col_names[-1]\n",
    "utils.print_array_hist(train_y, label=label_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the distribution of features look like?\n",
    "for i in range(train_x.shape[1]):\n",
    "    utils.print_array_basic_stats(train_x[:, i], label=col_names[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(copy=True)\n",
    "scaler.fit(all_x) \n",
    "\n",
    "scaled_all_x = scaler.transform(all_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's change the prediction task. Instead of prediction the quality score, we'll predict good vs. bad wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's modify the label to be consistent with the task: good = 1, bad = 0 (binary classification)\n",
    "numeric_y = all_y\n",
    "good_idx = np.where(numeric_y >= 6.0)[0]\n",
    "\n",
    "all_y = np.zeros_like(all_y).astype(int)\n",
    "all_y[good_idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's split the *scaled* data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now split between train, test, and validation!\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = utils.train_test_val_split(scaled_all_x, all_y, prop_vec, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train some bagging classifiers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Ref: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "\n",
    "# alternatively we could use random forests\n",
    "\n",
    "bagmodel = BaggingClassifier(DecisionTreeClassifier(), n_estimators=1000, max_samples=1500,\n",
    "                             bootstrap=True, n_jobs=-1, oob_score=True, random_state=seed)\n",
    "\n",
    "bagmodel.fit(train_x, train_y)\n",
    "\n",
    "# can we get an estimate of the performance on the test set without using the test set?\n",
    "# yes => out of the bag evaluation!\n",
    "print('[BaggingClassifier] Predicted test accuracy (oob score): {:.2f}%'.format(bagmodel.oob_score_*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is that estimate any good?\n",
    "train_acc = model_accuracy(bagmodel, train_x, train_y)\n",
    "val_acc = model_accuracy(bagmodel, val_x, val_y)\n",
    "test_acc = model_accuracy(bagmodel, test_x, test_y)\n",
    "\n",
    "print('[BaggingClassifier] Train accuracy: {:.2f}%, Val. accuracy: {:.2f}%, Test accuracy: {:.2f}%'.format(\n",
    "    train_acc*100, val_acc*100, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
