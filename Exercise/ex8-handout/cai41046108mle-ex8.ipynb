{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8: Training Neural Networks with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# we'll use keras for neural networks\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# import layers we will use\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D, Concatenate, Dropout\n",
    "\n",
    "# import callbacks we will use\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# Let's check our software versions\n",
    "print('### Python version: ' + sys.version)\n",
    "print('### Numpy version: ' + np.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('### Tensorflow version: ' + tf.__version__)\n",
    "print('------------')\n",
    "\n",
    "\n",
    "# load our packages / code\n",
    "sys.path.insert(1, '../common/')\n",
    "import utils\n",
    "import plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters to control behavior of the pre-processing, ML, analysis, etc.\n",
    "\n",
    "seed = 42 # deterministic seed\n",
    "np.random.seed(seed) \n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "prop_vec = [24, 2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use Fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_preprocess_fashion_mnist(minmax_normalize=True):\n",
    "    \n",
    "    labels = ['top', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    train, testval = fashion_mnist.load_data()\n",
    "    \n",
    "    train_x, train_y = train\n",
    "    testval_x, testval_y = testval\n",
    "    \n",
    "    if minmax_normalize:\n",
    "        train_x = train_x / 255.0\n",
    "        testval_x = testval_x / 255.0\n",
    "    \n",
    "    # split test - val\n",
    "    nval = testval_x.shape[0] // 2\n",
    "    \n",
    "    val_x = testval_x[:nval]\n",
    "    val_y = testval_y[:nval]\n",
    "    \n",
    "    test_x = testval_x[nval:]\n",
    "    test_y = testval_y[nval:]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_fashion_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 25\n",
    "label_idx = train_y[:num_images].astype(int)\n",
    "titles = labels[label_idx]\n",
    "plots.plot_images(train_x[:num_images].reshape(-1, 28, 28), dim_x=28, dim_y=28, fig_size=(9,9), titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_functional(input_shape=(28, 28)):  \n",
    "    \n",
    "    # let's use the functional API to create a model\n",
    "    input_layer = Input(shape=input_shape, name='Input')\n",
    "    # todo: fill in the blanks\n",
    "    \n",
    "    flatten_layer = Flatten(name='Flatten')(input_layer)\n",
    "    fc1 = Dense(300, name='FC1', activation='relu')(flatten_layer)\n",
    "    fc2 = Dense(100, name='FC2', activation='relu')(fc1)\n",
    "    output_layer = Dense(10, name='Output', activation='softmax')(fc2)\n",
    "    \n",
    "    model = keras.Model(name='FC-model', inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_functional()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the model look like?\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can examine the layers of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also query specific layers by name and get their weights\n",
    "fc1 = model.get_layer('FC1')\n",
    "\n",
    "weights, biases = fc1.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(biases[:10])\n",
    "print(weights[10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up an early stopping callback\n",
    "early_stop_cb = EarlyStopping(monitor='val_accuracy', patience=3)\n",
    "\n",
    "# set up a model checkpointing callback\n",
    "fp = \"./mymodel-bestweights.tf\"\n",
    "checkpoint_cb = ModelCheckpoint(fp, monitor='val_accuracy', save_best_only=True, mode='max')\n",
    "\n",
    "max_epochs = 100\n",
    "batch_size = 64\n",
    "history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=max_epochs, batch_size=batch_size, \n",
    "                     shuffle=True, callbacks=[early_stop_cb, checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is it different from the previous model?\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))\n",
    "\n",
    "loss, acc = loaded_model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Loaded Model] Test accuracy: {:.2f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can (of course) save the model directly (i.e., without checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mymodel.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip some layers with concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_skip(input_shape=(28, 28)):  \n",
    "    \n",
    "    # let's use the functional API to create a model\n",
    "    input_layer = Input(shape=input_shape, name='Input')\n",
    "    flatten_layer = Flatten(name='Flatten')(input_layer)\n",
    "    fc1 = Dense(324, activation='relu', name='FC1')(flatten_layer)\n",
    "    fc2 = Dense(128, activation='relu', name='FC2')(fc1)\n",
    "    \n",
    "    # let's concatenate the input (flattened) to the output of fc2\n",
    "    concat_layer = Concatenate(name='Concat')([fc2, flatten_layer]) \n",
    "    output_layer = Dense(10, activation='softmax', name='Output')(concat_layer)\n",
    "    \n",
    "    model = keras.Model(name='FC-model-with-skip', inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model_skip()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "early_stop_cb = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "max_epochs = 100\n",
    "batch_size = 128\n",
    "history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=max_epochs, batch_size=batch_size, \n",
    "                     shuffle=True, callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and compile model\n",
    "model = create_model_skip()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# set up tensorboard log directory and callback\n",
    "log_dir = './logs/fit/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_cb = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "# let's write some of the images data to logs\n",
    "new_log_dir = './logs/fit/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "fw = tf.summary.create_file_writer(new_log_dir)\n",
    "with fw.as_default():\n",
    "    for step in range(1, 11):\n",
    "        tf.summary.image('train_images_{}'.format(step), train_x[(step-1)].reshape(-1, 28, 28, 1), step=step)\n",
    "        \n",
    "\n",
    "max_epochs = 50\n",
    "batch_size = 128\n",
    "history = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=max_epochs, batch_size=batch_size, \n",
    "                     shuffle=True, callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard (notebook experience)\n",
    "%tensorboard --logdir ./logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can we tune hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use sklearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_model_with_hyperparams(skip=True, num_hidden=1, hidden_units=96, \n",
    "                                       activation_func='relu', eta=0.001, input_shape=(28, 28), verbose=False):\n",
    "    \n",
    "    if verbose:\n",
    "        print('Hyperparameters: ', num_hidden, hidden_units, activation_func, eta, skip)\n",
    "    \n",
    "    # define architecture\n",
    "    input_layer = Input(shape=input_shape, name='Input')\n",
    "    flatten_layer = Flatten(name='Flatten')(input_layer)\n",
    "        \n",
    "    fc1 = Dense(324, activation=activation_func, name='FC1')(flatten_layer)\n",
    "    \n",
    "    fcprev = fc1\n",
    "    for i in range(1, num_hidden):\n",
    "        fci = Dense(hidden_units, activation=activation_func, name='FC{}'.format(i+2))(fcprev)\n",
    "        fcprev = fci\n",
    "    fclast = fcprev\n",
    "    \n",
    "    if skip:\n",
    "        concat_layer = Concatenate(name='Concat')([fclast, flatten_layer]) \n",
    "        output_layer = Dense(10, activation='softmax', name='Output')(concat_layer)\n",
    "    else:\n",
    "        output_layer = Dense(10, activation='softmax', name='Output')(fclast)\n",
    "    \n",
    "    model = keras.Model(inputs=[input_layer], outputs=[output_layer])\n",
    "    \n",
    "    # compile the model\n",
    "    opt = keras.optimizers.Adam(learning_rate=eta)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can use sklearn wrapper to wrap the model into a sklearn estimator (so we can call fit, predict, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "wrapped_model = KerasClassifier(instantiate_model_with_hyperparams, skip=True, num_hidden=1, hidden_units=96, \n",
    "                                       activation_func='relu', eta=0.001, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can train the model using the wrapped model. For example:\n",
    "hist = wrapped_model.fit(train_x, train_y, epochs=2, validation_data=(val_x, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can call functions like score\n",
    "acc = wrapped_model.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do a hyperparameters search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do randomized search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# tuples otherwise there is a bug\n",
    "hyperparams_dist = {\n",
    "    'eta': (0.0001, 0.001, 0.01),\n",
    "    'skip': (True, False),\n",
    "    'num_hidden': (1, 2, 3, 4),\n",
    "    'hidden_units': (32, 64, 128, 256),\n",
    "    'activation_func': ('tanh', 'relu', 'selu')\n",
    "}\n",
    "\n",
    "cv=2\n",
    "n_iter=3\n",
    "random_search = RandomizedSearchCV(wrapped_model, hyperparams_dist, cv=cv, n_iter=n_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the actual search; observe the extra parameters we are passing to fit()\n",
    "# set up an early stopping callback\n",
    "early_stop_cb = EarlyStopping(patience=3)\n",
    "\n",
    "#verb=1\n",
    "verb=0\n",
    "_ = random_search.fit(train_x, train_y, epochs=10, validation_data=(val_x, val_y), callbacks=[early_stop_cb], verbose=verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning this could take a while\n",
    "random_search.best_params_, random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_compile_cnn(input_shape=[28, 28, 1], num_outputs=10, verbose=False):\n",
    "    \n",
    "    name = 'CNN'    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=(7,7), input_shape=input_shape,\n",
    "                     padding='same', activation='relu', name='conv1'))\n",
    "    model.add(MaxPooling2D(2, name='maxpool1')) \n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same', name='conv2'))\n",
    "    model.add(Conv2D(128, kernel_size=(3,3), activation='relu', padding='same', name='conv3'))\n",
    "    model.add(MaxPooling2D(2, name='maxpool2'))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size=(3,3), activation='relu', padding='same', name='conv4'))\n",
    "    model.add(Conv2D(256, kernel_size=(3,3), activation='relu', padding='same', name='conv5'))\n",
    "    model.add(MaxPooling2D(2, name='maxpool3'))\n",
    "    \n",
    "    model.add(Flatten(name='flatten'))\n",
    "    \n",
    "    model.add(Dense(128, activation='relu', name='fc1'))\n",
    "    model.add(Dropout(0.5, name='dropout1'))\n",
    "    model.add(Dense(64, activation='relu', name='fc2'))\n",
    "    model.add(Dropout(0.5, name='dropout2'))\n",
    "    \n",
    "    model.add(Dense(num_outputs, activation=\"softmax\", name='output'))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(lr=0.002)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, model = create_compile_cnn(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
