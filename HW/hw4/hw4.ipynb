{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, email and UFID.\n",
    "Please do not modify instruction cells or any cells with automated tests (marked with `[ASSERTS]`). Note: you can add new cells if you need them, but answers must be in the cells with `YOUR CODE HERE` or \"YOUR ANSWER HERE\" comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bd62aaf917895f4f6ddcc0d859f7f054",
     "grade": false,
     "grade_id": "homework-preamble",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Homework 4: Training Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5658c9a4ed21f362746fdaeb7fb21351",
     "grade": false,
     "grade_id": "preamble-name",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Preamble: Write your Name, Email and UFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8550e47617194707bd7501c6d716c85",
     "grade": false,
     "grade_id": "name-email-ufid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "NAME = 'Your name here.'\n",
    "EMAIL = 'Your email here.'\n",
    "UFID = 12345678\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('Homework 4 -- name: {}, email: {}, UFID: {}\\n'.format(NAME, EMAIL, UFID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c41a13a5f6e2fa1b47621d28e926c2cf",
     "grade": true,
     "grade_id": "name-email-ufid-asserts",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check that your name, email, and UFID is filled in.\"\"\"\n",
    "assert NAME != '' and NAME != 'Your name here.' and len(NAME) > 3\n",
    "assert EMAIL != '' and EMAIL != 'Your email here.' and len(EMAIL) > 7\n",
    "assert type(UFID) == int and UFID != 12345678 and UFID >= 10000000 and UFID <= 99999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b4f30c4bbc2f9927b1410eeb1f001b3",
     "grade": false,
     "grade_id": "preamble-academic-integrity",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Academic Integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99184dabc791053131230787b9f498b8",
     "grade": false,
     "grade_id": "preamble-academic-integrity-2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### <span style=\"color:red;\">This is an individual assignment. Academic integrity violations (i.e., cheating, plagiarism) will be reported to SCCR!</span><br/>\n",
    "#### The official CISE policy recommended for such offenses is a course grade of E. Additional sanctions may be imposed by SCCR such as marks on your permanent educational transcripts, dismissal or expulsion.\n",
    "#### Reminder of the Honor Pledge: On all work submitted for credit by Students at the University of Florida, the following pledge is either required or implied: *\"On my honor, I have neither given nor received unauthorized aid in doing this assignment.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d8d2452e1d9f6d547eae6447b7ca369",
     "grade": false,
     "grade_id": "cell-preamble-academic-integrity-3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Acknowledgement: Do you acknowledge and understand the academic integrity warning above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89bc9ed2e09cb9069b92dc24a3bc081a",
     "grade": false,
     "grade_id": "academic-integrity",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "academic_integrity_acknowledgement = False\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7d6eb103ab3a60e964c163468d9aa7a",
     "grade": true,
     "grade_id": "academic-integrity-assert",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check that you acknowledge the academic integrity warning, you understand it and have been reminded of the UF Honor Pledge.\"\"\"\n",
    "assert academic_integrity_acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40e0c6343e3d6ae8ef5568125e9de64f",
     "grade": false,
     "grade_id": "task1-instructc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The following cell's code (import statements etc.) is provided for you and you should not need to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a657110ad49635ac5f45bf6851cc7c0",
     "grade": false,
     "grade_id": "task1-code",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard\n",
    "\n",
    "\n",
    "# Let's check our software versions\n",
    "print('------------')\n",
    "print('### Python version: ' + __import__('sys').version)\n",
    "print('### NumPy version: ' + np.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('### Tensorflow version: ' + tf.__version__)\n",
    "print('------------')\n",
    "\n",
    "def var_exists(var_name):\n",
    "    return (var_name in globals() or var_name in locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cc6d3b909b447a027649ca1d83883d9",
     "grade": false,
     "grade_id": "seed_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### This is the seed we will use, do not change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e0b9db11795f3c51dea920123f1b8f49",
     "grade": false,
     "grade_id": "setting_seed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "epsf = 1e-9 # small epsilon value for floating point comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d746a78509c270bb1f51eff6a455a1f6",
     "grade": true,
     "grade_id": "seed_checking",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check seed. \"\"\"\n",
    "assert seed == 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "742d47f8db40e8977ab2c323db4af478",
     "grade": false,
     "grade_id": "task1-instructb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### We will use the Fashion-MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55e79f6c4e0ca8e58db9f96e844e8200",
     "grade": false,
     "grade_id": "task1-instructd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3c18bee960da611150f8280e8f7d37ad",
     "grade": false,
     "grade_id": "task1-loaddata",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "## Load and preprocess the Fashion-MNIST dataset\n",
    "\"\"\"\n",
    "def load_preprocess_fmnist_data(flatten=True, onehot=True, val_prop=0.5, seed=None, verbose=False):\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    \n",
    "    assert x_train.shape == (60000, 28, 28) and x_test.shape == (10000, 28, 28)\n",
    "    assert y_train.shape == (60000,) and y_test.shape == (10000,)\n",
    "\n",
    "    if verbose: \n",
    "        print('Loaded Fashion-MNIST data; shape: {} [y: {}], test shape: {} [y: {}]'.format(x_train.shape, y_train.shape,\n",
    "                                                                                      x_test.shape, y_test.shape))\n",
    "    \n",
    "    if flatten: # Let's flatten the images for easier processing (labels don't change)\n",
    "        flat_vec_size = 28*28\n",
    "        x_train = x_train.reshape(x_train.shape[0], flat_vec_size)\n",
    "        x_test = x_test.reshape(x_test.shape[0], flat_vec_size)\n",
    "\n",
    "    if onehot: # Put the labels in \"one-hot\" encoding using keras' to_categorical()\n",
    "        num_classes = 10\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    train_x = x_train\n",
    "    train_y = y_train\n",
    "\n",
    "    # we'll split the test set into test and val\n",
    "    testval_x = x_test\n",
    "    testval_y = y_test\n",
    "\n",
    "    # do the split\n",
    "    val_x, test_x, val_y, test_y = train_test_split(testval_x, testval_y, test_size=val_prop, random_state=seed)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y\n",
    "\n",
    "\n",
    "# grab the data\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_fmnist_data(flatten=True, onehot=True, val_prop=0.5, seed=seed) \n",
    "\n",
    "# sanity check shapes\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape)\n",
    "\n",
    "assert train_x.shape == (60000, 784) and train_y.shape == (60000,10) and test_x.shape == (5000, 784) and test_y.shape == (5000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d326223d72f27b0f38893641f74857a9",
     "grade": false,
     "grade_id": "task1-instructe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## First, let's setup some performance evaluation and plotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e14e33577156f3a1f9a346cece0aa638",
     "grade": false,
     "grade_id": "task1-providedcode1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a custom callback class\n",
    "class PerfEvalCustomCallback(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, perf_data):\n",
    "        self.perf_data = perf_data\n",
    "    \n",
    "    # we define the on_epoch_end callback and save the loss and accuracy in perf_data\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.perf_data[epoch,0] = logs['loss']\n",
    "        self.perf_data[epoch,1] = logs['accuracy']\n",
    "        self.perf_data[epoch,2] = logs['val_loss']\n",
    "        self.perf_data[epoch,3] = logs['val_accuracy']\n",
    "\n",
    "    def get_perf_data():\n",
    "        return self.perf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87ec89091901d11a6a013acbfe8c54d9",
     "grade": false,
     "grade_id": "task1-providedcode2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the model's performance during training (across epochs)\n",
    "def plot_training_perf(train_loss, train_acc, val_loss, val_acc, ax=None, fs=(6,3)):\n",
    "    no_ax_provided = ax == None\n",
    "    if no_ax_provided:\n",
    "        fig = plt.figure(figsize=fs)\n",
    "        ax = plt.gca()\n",
    "\n",
    "    assert train_loss.shape == val_loss.shape and train_loss.shape == val_acc.shape and val_acc.shape == train_acc.shape\n",
    "    \n",
    "    # assume we have one measurement per epoch\n",
    "    num_epochs = train_loss.shape[0]\n",
    "    epochs = np.arange(0, num_epochs)\n",
    "\n",
    "    ax.plot(1+epochs, train_acc, 'r--', linewidth=2, label='Training')\n",
    "    ax.plot(1+epochs, val_acc, 'b-', linewidth=2, label='Validation')\n",
    "    \n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    \n",
    "    ax.set_xlim([1, num_epochs])\n",
    "\n",
    "    ylim = [0.0, 1.01]\n",
    "    ax.set_ylim(ylim)\n",
    "\n",
    "    ax.legend()\n",
    "    if no_ax_provided:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b58b14debfa83e3c6591cd40dcf9560",
     "grade": false,
     "grade_id": "task1-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 1] (15 points) Complete (& Customize) Your Model Training and Evaluation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ecd36fd84ec27f5cb609dbfbb92d70f",
     "grade": false,
     "grade_id": "task1-instruct1a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 1a] (10 points) Complete the implementation of evaluate_model(). You can customize it to add whatever evaluation functionality you like (e.g., classification report, error analysis, etc.). Make sure you like the way it shows you the information: you will use it throughout this homework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bc60e9301daac2b63ebd9dd4c8960cd",
     "grade": false,
     "grade_id": "task1a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in your code in the function (~6+ lines)\n",
    "\"\"\"\n",
    "# Customize this function as you like but makes sure it is implemented correctly.    \n",
    "# Note: If you need to change the method definition to add more arguments, make sure to make \n",
    "# the new arguments are optional (& have a sensible default value)\n",
    "def evaluate_model(name, model, eval_data, \n",
    "                   plot_training=True, evaluate_on_test_set=True):\n",
    "    \n",
    "    # unpack the stuff\n",
    "    perf_data, dataset = eval_data\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = dataset\n",
    "    \n",
    "    # get predictions from the model\n",
    "    train_preds = model.predict(train_x, verbose=0)\n",
    "    val_preds = model.predict(val_x, verbose=0)\n",
    "    \n",
    "    # measure the accuracy (as categorical accuracy since we have a softmax layer)\n",
    "    \"\"\" use keras.metrics.CategoricalAccuracy() to measure the accuracy on the training set ('train_acc') and val set ('val_acc') \n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    assert np.isnan(train_acc) or (float(train_acc) == train_acc and train_acc >= 0 and train_acc < 1.0)\n",
    "    assert np.isnan(val_acc) or (float(val_acc) == val_acc and val_acc >= 0 and val_acc < 1.0)\n",
    "    \n",
    "    print('[{}] Training Accuracy: {:.3f}%, Validation Accuracy: {:.3f}%'.format(name, 100*train_acc, 100*val_acc))\n",
    "        \n",
    "    if evaluate_on_test_set:\n",
    "        ### Evaluate the model on test data and put the results in 'test_loss', 'test_acc' (set verbose = 0)\n",
    "        \"\"\" Use model.evaluate() to measure the loss and accuracy on the *test* set ('test_loss' and 'test_acc').\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        assert np.isnan(test_loss) or (test_loss == float(test_loss) and test_loss >= -1e8)\n",
    "        assert np.isnan(test_acc) or (test_acc == float(test_acc) and test_acc >= 0 and test_acc < 1.0)\n",
    "    \n",
    "        print('[{}] Test loss: {:.5f}, test accuracy: {:.3f}%'.format(name, test_loss, 100*test_acc))\n",
    "\n",
    "    if plot_training:\n",
    "        plot_training_perf(perf_data[:,0], perf_data[:,1], perf_data[:,2], perf_data[:,3])\n",
    "    \n",
    "    # You can add stuff here if you want\n",
    "    ###* put any additional code here (0+ lines) *###\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return\n",
    "\n",
    "## this is what we call to do the training\n",
    "def train_model(model, max_epochs=25, batch_size=128, verbose=0, \n",
    "                   dataset=(train_x, train_y, val_x, val_y, test_x, test_y)):\n",
    "\n",
    "    ## unpack dataset\n",
    "    train_x, train_y, val_x, val_y, test_x, test_y = dataset\n",
    "    \n",
    "    ## this is the callback we'll use for early stopping\n",
    "    early_stop_cb = keras.callbacks.EarlyStopping(monitor='loss', mode='min', patience=40)\n",
    "    \n",
    "    ## setup the performance data callback\n",
    "    perf_data = np.zeros((max_epochs, 4))\n",
    "    perf_eval_cb = PerfEvalCustomCallback(perf_data)\n",
    "    \n",
    "    hobj = model.fit(train_x, train_y, validation_data=(val_x, val_y), epochs=max_epochs, batch_size=batch_size, \n",
    "                     shuffle=True, callbacks=[perf_eval_cb, early_stop_cb], verbose=verbose)\n",
    "    \n",
    "    eff_epochs = len(hobj.history['loss'])\n",
    "    eval_data = (perf_data[0:eff_epochs,:], dataset) ## tuple of evaluation data\n",
    "    \n",
    "    return eval_data\n",
    "\n",
    "\n",
    "def compile_train_eval_compare(create_compile_fn):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(13,4))\n",
    "\n",
    "    ax1 = axes[0]\n",
    "    ax1.set_title('Broken')\n",
    "    broken_name, broken_model = create_compile_fn(False, verbose=False) # create & compile (fixed=False)\n",
    "    broken_eval_data = train_model(broken_model)\n",
    "    broken_perf_data, _ = broken_eval_data\n",
    "    plot_training_perf(broken_perf_data[:,0], broken_perf_data[:,1], broken_perf_data[:,2], broken_perf_data[:,3], ax=ax1)\n",
    "\n",
    "    ax2 = axes[1]\n",
    "    ax2.set_title('Fixed')\n",
    "    fixed_name, fixed_model = create_compile_fn(True, verbose=False) # create & compile  (fixed=True)\n",
    "    fixed_eval_data = train_model(fixed_model)\n",
    "    fixed_perf_data, _ = fixed_eval_data\n",
    "    plot_training_perf(fixed_perf_data[:,0], fixed_perf_data[:,1], fixed_perf_data[:,2], fixed_perf_data[:,3], ax=ax2)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return broken_name, broken_model, broken_eval_data, broken_perf_data, fixed_name, fixed_model, fixed_eval_data, fixed_perf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74ce0bbb497e6fb71aa41e90ee29ac01",
     "grade": true,
     "grade_id": "task1a-tests",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 1a completed. \"\"\"\n",
    "assert var_exists('evaluate_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70023afe1613ec83bd9e9851670f41ea",
     "grade": false,
     "grade_id": "task2-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 2] (30 points) Diagnosing Simple Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d22851fa04c164a20d7049dfec63f5f",
     "grade": false,
     "grade_id": "task2-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## In the following Tasks (Tasks 2 & 3) you are given code to define the architecture of a model and compile it. But there is an issue for each model (it's broken in some way), which you need to identify and fix. All the models (if fixed) should achieve 85%+ val/test accuracy\n",
    "\n",
    "## To diagnose the issue you need to observe the broken model's training process. Then you need to fix it by making *minimal* changes. We are not looking for changes that completely alter the architecture for example (like adding a bunch of new layers or removing many layers, etc.). You will add 'if fixed:' branches in the code to implement the fix without modify the behavior in anyway whenever 'fixed=False'. \n",
    "\n",
    "## Important: The assertions and hidden tests will ensure that the behavior is as expected when your code is invoked both for fixed = False and fixed = True, so be careful not to make unintended modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63e15bcfeecfb23aee66d2d7d7777e6d",
     "grade": false,
     "grade_id": "task2a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2a] (10 points) Consider the following model. It has one obvious problem which prevents the model from learning: the output layer's activation function and the loss are inconsistent. Fix it by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation!\n",
    "\n",
    "### Hint: since the output layer's activation function is softmax (which makes sense since we have one-hot encoded class labels), the loss should be the cross entropy loss (see tf.keras losses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77368479c3be038b907a292c19c5743c",
     "grade": false,
     "grade_id": "task2a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_compile_model0(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    name = 'Model0--Fixed' if fixed else 'Model0--Broken'\n",
    "    hidden_widths=[300, 100]\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(keras.Input(shape=(input_shape,), sparse=False)) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(keras.layers.Dense(hw, activation='relu', name='hidden_{}'.format(i), \n",
    "                                     kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(1/hw)),\n",
    "                                     bias_initializer=keras.initializers.Zeros()))\n",
    "        \n",
    "    model.add(keras.layers.Dense(num_outputs, activation='softmax', name='output',\n",
    "                                kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(0.1)),\n",
    "                                bias_initializer=keras.initializers.Zeros()))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.002)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    else:\n",
    "        model.compile(loss='mse', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41b992e1b07c73858f3a7b602cd944a4",
     "grade": false,
     "grade_id": "task2a-provided-code",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## create and compile the model for fixed=False, train it, then evaluate it\n",
    "fixed = False\n",
    "name, model = create_compile_model0(fixed) \n",
    "\n",
    "# train\n",
    "eval_data = train_model(model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "797f7d8971e0207dfe7c42e7c3dbebdf",
     "grade": false,
     "grade_id": "task2a-provided-code2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4c9a632e5375ea3887321397df4d1094",
     "grade": false,
     "grade_id": "task2a-provided2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Now let's check if you fixed the issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32f658a12b8e31725b5dfffe418fcb88",
     "grade": false,
     "grade_id": "task2a-provided-checking",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## create and compile the model for fixed=True, train it, then evaluate it\n",
    "fixed = True\n",
    "name, model = create_compile_model0(fixed, verbose=True) \n",
    "\n",
    "# train\n",
    "eval_data = train_model(model, verbose=1)\n",
    "\n",
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8494f793f81daa9f46a6173ff9dcf86f",
     "grade": true,
     "grade_id": "task2a-checks",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 2a completed. \"\"\"\n",
    "\n",
    "_, broken_model, _, broken_perf_data, _, fixed_model, _, fixed_perf_data = compile_train_eval_compare(create_compile_model0)\n",
    "assert len(broken_model.layers) == len(fixed_model.layers) # check the layers have not changed (hint: layers are fine)\n",
    "\n",
    "broken_tracc = broken_perf_data[:,1]\n",
    "broken_valacc = broken_perf_data[:,3]\n",
    "fixed_tracc = fixed_perf_data[:,1]\n",
    "fixed_valacc = fixed_perf_data[:,3]\n",
    "\n",
    "assert np.amax(broken_valacc) < 0.3 and np.amax(fixed_valacc) > 0.75 # loose threshold to check that it was fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1be7d2b91ace71e1216053b42141a583",
     "grade": false,
     "grade_id": "task2a-conc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Does the model works as expected now? Does it achieve about 85% accuracy (or at least 80%)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1c72d11d9129c61d931078479314edd",
     "grade": false,
     "grade_id": "task2b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2b] (5 points) Just like task 2a, consider the following model. It has one obvious problem which prevents the model from learning: can you figure out what it is? Fix it by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation!\n",
    "\n",
    "### Note: the model is different in some ways to model0 but most of these ways are unrelated to the problem you are asked to diagnose and fix!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "509763eec30b226aecbda380b3a36483",
     "grade": false,
     "grade_id": "task2b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_compile_model1(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    \n",
    "    \"\"\" Remove the raise NotImplementedError lines, then move the if fixed branch to where you want to use it, and create a corresponding else (if necessary) to preserve \n",
    "    the broken functionality when fixed=False. For example:\n",
    "    #if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "    Note: the 'raise NotImplementedError' lines may not match where you should put your 'if fixed' branch.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    name = 'Model1--Fixed' if fixed else 'Model1--Broken'\n",
    "    hidden_widths=[256, 128, 48]\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(keras.Input(shape=(input_shape,), sparse=False)) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(keras.layers.Dense(hw, activation='relu', name='hidden_{}'.format(i), \n",
    "                                     kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(1/hw)),\n",
    "                                     bias_initializer=keras.initializers.Zeros(), use_bias=False))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    model.add(keras.layers.Dense(num_outputs, activation='softmax', name='output',\n",
    "                                kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(0.1)),\n",
    "                                bias_initializer=keras.initializers.Zeros(), use_bias=True))\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.1)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f046d4572e22682825cc702aa9ad7d02",
     "grade": false,
     "grade_id": "task2b-provided1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## create and compile the model for fixed=False, train it, then evaluate it\n",
    "fixed = False\n",
    "name, model = create_compile_model1(fixed) \n",
    "\n",
    "# train the model (hint you can set verbose to 1 to get more information during the training process)\n",
    "eval_data = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f5897dbd74eb80900faf740f8541de33",
     "grade": false,
     "grade_id": "task2b-provided2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a1ecb97d10974026d7c4943880577cd",
     "grade": false,
     "grade_id": "cell-a1be3f1d92c21eca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's check if you fixed the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c367053139691393c43390788877db5b",
     "grade": false,
     "grade_id": "task2b-provided3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fixed = True\n",
    "name, model = create_compile_model1(fixed, verbose=False) \n",
    "\n",
    "eval_data = train_model(model)\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7a08ae1bf54090f6cd2a620a767ab3f",
     "grade": true,
     "grade_id": "task2b-tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 2b completed. \"\"\"\n",
    "\n",
    "assert var_exists('create_compile_model1') # note: hidden tests will check accuracy is high enough for Fixed=True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "994a899d37432592b83d38f409545c69",
     "grade": false,
     "grade_id": "task2c-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2c] (5 points) Explain what was the problem. (A sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba2ed055923b395bdbd6d3d8a6a4f2bd",
     "grade": true,
     "grade_id": "task2c-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1b2649d640fe5f418288b8d1e808e1b",
     "grade": false,
     "grade_id": "task2d-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2d] (5 points) Just like task 2a and task 2b, consider the following model. It has one obvious problem which prevents the model from learning: can you figure out what it is? Fix it by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6c5fbd411de28ff2f825567ae03113e3",
     "grade": false,
     "grade_id": "task2d-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_compile_model2(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    \n",
    "    \"\"\" Remove the raise NotImplementedError lines, then move the if fixed branch to where you want to use it, and create a corresponding else (if necessary) to preserve \n",
    "    the broken functionality when fixed=False. For example:\n",
    "    #if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "    Note: the 'raise NotImplementedError' lines may not match where you should put your 'if fixed' branch.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    name = 'Model2--Fixed' if fixed else 'Model2--Broken'\n",
    "    hidden_widths=[400, 96]\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(keras.Input(shape=(input_shape,), sparse=False)) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(keras.layers.Dense(hw, activation='relu', name='hidden_{}'.format(i), \n",
    "                                     use_bias=True))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    model.add(keras.layers.Dense(num_outputs, activation='softmax', name='output', use_bias=False))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    opt = keras.optimizers.RMSprop(learning_rate=1e-8)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd581bce7cc7dc0c687b0f40746d278e",
     "grade": false,
     "grade_id": "task2d-provided1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create and compile the model for fixed=False, train it, then evaluate it\n",
    "fixed = False\n",
    "name, model = create_compile_model2(fixed) \n",
    "\n",
    "# train the model (hint you can set verbose to 1 to get more information during the training process)\n",
    "eval_data = train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df48c384f550e68c4fc008441c1ecb0a",
     "grade": false,
     "grade_id": "task2d-provided2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fff43f7b28389f0a6daa7a0b6874b2eb",
     "grade": false,
     "grade_id": "cell-952e2c850f80bce5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's check if you fixed the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8e2eb49722eb4339a500809a343292a",
     "grade": false,
     "grade_id": "task2d-provided3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fixed = True\n",
    "name, model = create_compile_model2(fixed, verbose=False)\n",
    "\n",
    "eval_data = train_model(model)\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "afb5483507c06f60d132467f12497805",
     "grade": true,
     "grade_id": "task2d-answer",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 2d completed. \"\"\"\n",
    "\n",
    "assert var_exists('create_compile_model2') # note: hidden tests will check accuracy is high enough for Fixed=True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d1a0295aaaf554c8cffc9a3de21b106",
     "grade": false,
     "grade_id": "task2e-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2e] (5 points) Explain what was the problem. (A sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a06968bd62338a66a8caad6f13c85b2a",
     "grade": true,
     "grade_id": "task2e-manual-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2dd60fc2503571107d85f99ae873b121",
     "grade": false,
     "grade_id": "task3-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 3] (30 points) Diagnosing Other Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "75b649e8f19f3b7da931f988451dee3c",
     "grade": false,
     "grade_id": "task3a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 3a] (5 points) Just like in task 2, consider the following model. It has one obvious problem which prevents the model from learning: can you figure out what it is? Fix it by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "549684a7c603849c6d552c58136b07f9",
     "grade": false,
     "grade_id": "task3a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_compile_model3(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    \n",
    "    \"\"\" Remove the raise NotImplementedError lines, then move the if fixed branch to where you want to use it, and create a corresponding else (if necessary) to preserve \n",
    "    the broken functionality when fixed=False. For example:\n",
    "    #if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "    Note: the 'raise NotImplementedError' lines may not match where you should put your 'if fixed' branch.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    name = 'Model3--Fixed' if fixed else 'Model3--Broken'\n",
    "    hidden_widths=[512, 128, 32, 24]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(keras.Input(shape=(input_shape,), sparse=False)) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(keras.layers.Dense(hw, activation='relu', name='hidden_{}'.format(i), \n",
    "                                     bias_initializer=keras.initializers.RandomNormal(stddev=0.001), use_bias=True))\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    if not fixed:\n",
    "        model.add(keras.layers.Dense(num_outputs, activation='tanh', name='output',\n",
    "                                kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(0.1)),\n",
    "                                bias_initializer=keras.initializers.Zeros(), use_bias=True))\n",
    "    \n",
    "        opt = keras.optimizers.Adam(learning_rate=0.0004, beta_1=0.9, beta_2=0.995, epsilon=1e-07, amsgrad=False)\n",
    "        \n",
    "        if verbose:\n",
    "            model.summary()\n",
    "        \n",
    "        model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb76282ff44babbd416fc779ed34dc89",
     "grade": false,
     "grade_id": "task3a-provided1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create and compile the model for fixed=False, train it, then evaluate it\n",
    "name, model = create_compile_model3(False) \n",
    "\n",
    "# train the model (hint you can set verbose get more or less information during the training process)\n",
    "eval_data = train_model(model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9889f4ee87c56779c91dc348dfb0727c",
     "grade": false,
     "grade_id": "task3a-provided2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc17c090ed9965d97d2f7cfefa9d8bd2",
     "grade": false,
     "grade_id": "cell-8f29086e30eaf70b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's check if you fixed the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "777857a89377f09b8d04d328660ec7aa",
     "grade": false,
     "grade_id": "task3a-provided3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "name, model = create_compile_model3(True, verbose=False) \n",
    "\n",
    "eval_data = train_model(model)\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d307badd8a669443b606969b47cf12fc",
     "grade": true,
     "grade_id": "task3a-tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 3a completed. \"\"\"\n",
    "\n",
    "assert var_exists('create_compile_model3') # note: hidden tests will check accuracy is high enough for Fixed=True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec3b269b309d6215ee1761469dab74ef",
     "grade": false,
     "grade_id": "task3b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "###  [Task 3b] (5 points) Explain what was the problem. (One sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4e2ad9c0e196253db90600ddb883949",
     "grade": true,
     "grade_id": "task3b-manual-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2412c2b68e3f04f9c9ef44580d4ef875",
     "grade": false,
     "grade_id": "task3c-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 3c] (5 points) Consider the following model. It has one obvious problem which prevents the model from learning: can you figure out what it is? Fix it by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dc8e32d7b9102b9a01767b9854b97e25",
     "grade": false,
     "grade_id": "task3c-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_compile_model4(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    \n",
    "    \"\"\" Remove the raise NotImplementedError lines, then move the if fixed branch to where you want to use it, and create a corresponding else (if necessary) to preserve \n",
    "    the broken functionality when fixed=False. For example:\n",
    "    #if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "    Note: the 'raise NotImplementedError' lines may not match where you should put your 'if fixed' branch.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    name = 'Model4--Fixed' if fixed else 'Model4--Broken'\n",
    "    hidden_widths=[512, 128, 3, 24]\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "    \n",
    "    model.add(keras.Input(shape=(input_shape,), sparse=False)) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        model.add(keras.layers.Dense(hw, activation='relu', name='hidden_{}'.format(i)))\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    model.add(keras.layers.Dense(num_outputs, activation='softmax', name='output',\n",
    "                                kernel_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(0.1)),\n",
    "                                bias_initializer=keras.initializers.Zeros(), use_bias=True))\n",
    "    \n",
    "    opt = keras.optimizers.Nadam(learning_rate=0.001)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56ef5ec29c1a9be145ad4f05bf42b330",
     "grade": false,
     "grade_id": "task3c-provided1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create and compile the model for fixed=False, train it, then evaluate it\n",
    "name, model = create_compile_model4(False) \n",
    "\n",
    "# train the model (hint you can set verbose get more or less information during the training process)\n",
    "eval_data = train_model(model, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92120c1e20812e88cd0aafbc6058d40a",
     "grade": false,
     "grade_id": "task3c-provided2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a787177c34fb6f7ee409a6c919084340",
     "grade": false,
     "grade_id": "cell-8cd64c9758353040",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's check if you fixed the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "07a04aa6e3b2a58fe9e224b6370fb2ef",
     "grade": false,
     "grade_id": "task3c-provided3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "name, model = create_compile_model4(True, verbose=False) \n",
    "\n",
    "eval_data = train_model(model)\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6236e816ad6bc32ee9418a9ca6771a7e",
     "grade": true,
     "grade_id": "task3c-tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 3c completed. \"\"\"\n",
    "\n",
    "assert var_exists('create_compile_model4') # note: hidden tests will check accuracy is high enough for Fixed=True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cff64e6908377e1ea54559bc8d920e79",
     "grade": false,
     "grade_id": "task3d-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 3d] (5 points) Explain what was the problem. (A sentence or two is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e675bd78c98b878c2b6388fe9359bf39",
     "grade": true,
     "grade_id": "task3d-manual-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9955b6b690e409163c1026aa503e13a5",
     "grade": false,
     "grade_id": "task3e-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 3e] (5 points) Consider the following model. It has several problems which prevent it from learning: can you figure out what those problems are? Fix them by adding code to the 'if fixed:'' branch. Make sure that if fixed=False you do not change the implementation! You can have more than one \"if fixed:\" branch if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d5536e216252e20fa27e7e8813bde5e8",
     "grade": false,
     "grade_id": "task3e-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_compile_model5(fixed, input_shape=784, num_outputs=10, verbose=True):\n",
    "    \n",
    "    \n",
    "    \"\"\" Remove the raise NotImplementedError lines, then move the if fixed branch to where you want to use it, and create a corresponding else (if necessary) to preserve \n",
    "    the broken functionality when fixed=False. For example:\n",
    "    #if fixed:\n",
    "        ###* put your code here (~1-2 lines) *###\n",
    "\n",
    "    Note: the 'raise NotImplementedError' lines may not match where you should put your 'if fixed' branch.\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    name = 'Model5--Fixed' if fixed else 'Model5--Broken'\n",
    "    hidden_widths=[256, 128, 24]\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    if not fixed:\n",
    "        model.add(keras.Input(shape=(input_shape,))) \n",
    "    \n",
    "    for i, hw in enumerate(hidden_widths):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        if not fixed:\n",
    "            model.add(keras.layers.Dense(hw, activation='sigmoid', name='hidden_{}'.format(i)))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    if not fixed:\n",
    "        model.add(keras.layers.Dense(num_outputs, activation='linear', name='output',\n",
    "                                bias_initializer=keras.initializers.RandomNormal(stddev=np.sqrt(0.1)),\n",
    "                                kernel_initializer=keras.initializers.Zeros(), use_bias=False))\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    opt = keras.optimizers.Nadam(learning_rate=0.01, beta_2=0.999, epsilon=1e-07, clipvalue=1.0)\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b16e89de55466e6af2385f7584d27e6",
     "grade": false,
     "grade_id": "task3e-provided1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# create and compile the model for fixed=False, train it, then evaluate it\n",
    "name, model = create_compile_model5(False) \n",
    "\n",
    "# train the model (hint you can set verbose get more or less information during the training process)\n",
    "eval_data = train_model(model, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cb42b12b2370ca5939f6feae61881813",
     "grade": false,
     "grade_id": "task3e-provided2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2db1a67c19424fce7d1613da847bd9a",
     "grade": false,
     "grade_id": "cell-8a677dc7be25a170",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's check if you fixed the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30abf865018f5c71cd5a42aaae4d9633",
     "grade": false,
     "grade_id": "task3e-provided3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "name, model = create_compile_model5(True, verbose=False) \n",
    "\n",
    "eval_data = train_model(model)\n",
    "evaluate_model(name, model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b3acb2e22de71a40bce8c2a4f10b743d",
     "grade": true,
     "grade_id": "task3e-tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 3e completed. \"\"\"\n",
    "\n",
    "assert var_exists('create_compile_model5') # note: hidden tests will check accuracy is high enough for Fixed=True.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca6347a8d044bfe042efdd779f09f2f8",
     "grade": false,
     "grade_id": "task3f-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 3f] (5 points) Explain what was the problem. If there are multiple issues, describe all of them. (A sentence or two for each issue is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "942d14f32e1ced5acc22573d2dbded51",
     "grade": true,
     "grade_id": "task3f-manual-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e76b6d9ca11bc51ee677d2d659cfb9ce",
     "grade": false,
     "grade_id": "task4-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 4] (25 points) Training CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c27cfeb4411f720c8dd4fa14c172da4",
     "grade": false,
     "grade_id": "task4-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### In this task, you will train a convolutional neural network with an architecture you define. The goal is (of course) to train the best possible model, but the constraint is that the number of parameters must not exceed 500k (500,000). We will aim to achieve test/val accuracy above 88%.\n",
    "\n",
    "### If you do this on a machine with a GPU, it will be very fast. Otherwise it may take 5 or 10 minutes to train the model for a few epochs, so we will use a subset of the training data for developing and testing the model.\n",
    "\n",
    "#### Note: it is possible to achieve reasonably high accuracy (above 90%) with only about 50k parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d5e270ad87b210634f3757f7e4eb2d1",
     "grade": false,
     "grade_id": "cell-c42e070c9b6d3fb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### To use a convolutional architecture we need to reshape the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "133f07f71d3f3b7ebdfaa32764ab3716",
     "grade": false,
     "grade_id": "task4-initial-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "use_subset = True\n",
    "tr_sz = 5000\n",
    "\n",
    "#use_subset = False # uncomment this line to use the full data\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# reshape for use with CNN\n",
    "train_x = train_x.reshape(-1, 28, 28, 1)\n",
    "val_x = val_x.reshape(-1, 28, 28, 1)\n",
    "test_x = test_x.reshape(-1, 28, 28, 1)\n",
    "\n",
    "cnn_dataset = (train_x, train_y, val_x, val_y, test_x, test_y)\n",
    "if use_subset:\n",
    "    cnn_dataset = (train_x[:tr_sz], train_y[:tr_sz], val_x, val_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e82447eb98152f7ce5455c8c30344a83",
     "grade": false,
     "grade_id": "task4a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 4a] (15 points) Fill in the implementation of create_compile_cnn() below with your chosen architecture. Ensure the total number of parameters chosen does not exceed 500k. You can look for insipiration for architectures on the web (or in books) if you like, but in that case you must provide a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c3714deb664d6f8359bef1d4fa82650",
     "grade": false,
     "grade_id": "task4a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_compile_cnn(input_shape=(28, 28, 1), num_outputs=10, verbose=False):\n",
    "    name = 'CNN'\n",
    "    assert train_x.shape[1:] == input_shape # sanity check\n",
    "    \n",
    "    model = keras.models.Sequential(name=name)\n",
    "\n",
    "    \"\"\" Put your code here (10-20 lines).\n",
    "    You can use any architecture but the output layer (with softmax activation is fixed). \n",
    "    You can also use whatever optimizer 'opt' you want (with your choice hyperparameters values)\n",
    "    But the call to model.summary() must show that the number of parameters is within the constraint.\n",
    "    \"\"\"\n",
    "    ### Note: you can import layers from keras.layers to make the code more compact.\n",
    "    ###* put your code here (~10-20 lines) *###\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return name, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1728d898e40200bcc6fae12afd231d86",
     "grade": false,
     "grade_id": "task4a-provided1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "name, cnn_model = create_compile_cnn(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a07845d2dfae276de95efa4d65638f5e",
     "grade": false,
     "grade_id": "task4a-provided-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# note: you can change the number of epochs to train as long as it's reasonable\n",
    "epochs = 10 if use_subset else 5\n",
    "\n",
    "\"\"\" You can add code here or edit it to change the number of epochs for example or the batch size, etc.\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# note: you can change how you call train_model as necessary (e.g., if you want to change the batch_size)\n",
    "eval_data = train_model(cnn_model, max_epochs=epochs, dataset=cnn_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12e9fb9797ef1beb339064b03974c2ba",
     "grade": false,
     "grade_id": "task4a-provided2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Let's evaluate your CNN model. Does it achieve 88+% val/test accuracy?\n",
    "#### *note: once you are confident in your model architecture, you MUST switch back to the full dataset*\n",
    "#### to train the final version of the model (depending on your chosen architecture this could take 5-10 minutes if you are not on machine with GPU)\n",
    "#### make sure it does not take too long to train the model as this could be a problem for grading (e.g., <10min on a machine with a CPU only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe3bcff6f0a27f230a94ff9800c8f5ed",
     "grade": false,
     "grade_id": "task4a-provided3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "evaluate_model(name, cnn_model, eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b0daab9756ff0dc88d5ce573e074572",
     "grade": true,
     "grade_id": "task4a-tests",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import layer_utils\n",
    "\n",
    "\"\"\" [ASSERTS] Check 4a completed. \"\"\"\n",
    "\n",
    "assert var_exists('cnn_model') and isinstance(cnn_model, keras.Model)\n",
    "trainable_count = layer_utils.count_params(cnn_model.trainable_weights)\n",
    "assert trainable_count <= 500 * 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2061f210f712d02688d867d3559b77f3",
     "grade": false,
     "grade_id": "task4b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 4b] (5 points) How long (in seconds/minutes) did your model take to train on the full dataset? Did you use a GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "550c4641d8783a05f1a220fab51601c1",
     "grade": true,
     "grade_id": "task4b-manual-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ea659879f1241517f16966d1cf0399b",
     "grade": false,
     "grade_id": "task4c-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 4c] (5 points) Explain how you decided on this architecture. (2-3 sentences is okay.) If you took inspiration from resources such as books/webpages, it's okay but you should (of course) include a reference in your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "499839cba000d1f1d9195c26430f1df9",
     "grade": true,
     "grade_id": "task4c-manual-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23a79bae6cbec2603653c5c5f07e59e6",
     "grade": false,
     "grade_id": "task5-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 5] \\<*For CAI6108MLE Only*\\> (25 points) CNN vs. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9484560447fdd17820719882dc5e0275",
     "grade": false,
     "grade_id": "task5-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### In this task we want to understand the impact of data augmentation on model quality. We pretend that we only have access to 2k examples from the training set and then multiply its size (by 5X) using data augmentation.\n",
    "\n",
    "### The idea is to compare the performance of three models (all of which using your CNN architecture from Task 4): (1) the CNN trained on tr_x, ty_y (2k examples), (2) the CNN trained on the augmented data (20k examples), and (3) the CNN trained on 20k examples from the fashion-MNIST training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4d9cac49ea7177543a1a5891cf5f554",
     "grade": false,
     "grade_id": "task5a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 5a] (15 points) Complete the code below to use data augmentation to produce a dataset of size 'data_aug_sz' from the tr_x and tr_y. You can use tf.keras' ImageDataGenerator with parameters chosen by you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dea4b7acad0a2cb21f1f3c40b3a736d",
     "grade": false,
     "grade_id": "task5a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# dataset 1 -- 2k examples from fmnist\n",
    "tr_sz = 2000\n",
    "cnn_dataset1 = (train_x[:tr_sz], train_y[:tr_sz], val_x, val_y, test_x, test_y)\n",
    "\n",
    "# dataset 2 -- data augmentation (20k) starting from 2k examples from fmnist\n",
    "tr_x = train_x[:tr_sz]\n",
    "tr_y = train_y[:tr_sz]\n",
    "\n",
    "data_aug_sz = 20000\n",
    "\n",
    "\"\"\" Put your code here (10-15 lines).\n",
    "\"\"\"\n",
    "### Note: you should look closely at the documentation of ImageDataGenerator (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)\n",
    "### to make sure you do *not* do strange things (e.g., it might not be a good idea to have vertical_flip=True, etc.)...\n",
    "### Store the augmented data into 'aug_tr_x' and 'aug_tr_y'\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# store the result\n",
    "cnn_dataset2 = (aug_tr_x, aug_tr_y, val_x, val_y, test_x, test_y)\n",
    "assert aug_tr_x.shape[0] == data_aug_sz and aug_tr_y.shape[0] == data_aug_sz\n",
    "\n",
    "# dataset 3 -- 20k examples\n",
    "cnn_dataset3 = (train_x[:data_aug_sz], train_y[:data_aug_sz], val_x, val_y, test_x, test_y)\n",
    "assert aug_tr_x.shape == cnn_dataset3[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f58460d6752273301bbcee625c242faf",
     "grade": true,
     "grade_id": "task5a-tests",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Now let's evaluate all three models\n",
    "\n",
    "datasets = (cnn_dataset1, cnn_dataset2, cnn_dataset3)\n",
    "names = ['Model 1 (train size: {})'.format(tr_sz), 'Model 2 Data Augmented (train size: {})'.format(data_aug_sz),\n",
    "         'Model 3 (train size: {})'.format(data_aug_sz)]\n",
    "\n",
    "for i, dataset in enumerate(datasets):\n",
    "    name, cnn_model = create_compile_cnn(verbose=False)\n",
    "\n",
    "    # note: you can change how you call train_model as necessary (e.g., if you want to change the batch_size)\n",
    "    eval_data = train_model(cnn_model, max_epochs=15, dataset=dataset, verbose=0)  \n",
    "\n",
    "    print('\\n---------- {} -----------'.format(names[i]))\n",
    "    evaluate_model(name, cnn_model, eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f02ddb845eaa35016dde90cc510b10ad",
     "grade": false,
     "grade_id": "task5b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 5b] (10 points) What do you conclude about data augmentation? Does it help if you don't have enough data? Is it as good as having more real data? (A few sentences is fine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78b64601fc57555bc445f0557e909efe",
     "grade": true,
     "grade_id": "task5b-manual-answer",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
