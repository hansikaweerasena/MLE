{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name, email and UFID.\n",
    "Please do not modify instruction cells or any cells with automated tests (marked with `[ASSERTS]`). Note: you can add new cells if you need them, but answers must be in the cells with `YOUR CODE HERE` or \"YOUR ANSWER HERE\" comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fd969e84ec927900cb906419a0a37f33",
     "grade": false,
     "grade_id": "homework-preamble",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Homework 5: CNNs, RNNs, and AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5658c9a4ed21f362746fdaeb7fb21351",
     "grade": false,
     "grade_id": "preamble-name",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Preamble: Write your Name, Email and UFID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e6a10079f403d3924cf4fa5fcf3509ad",
     "grade": false,
     "grade_id": "name-email-ufid",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "NAME = 'Hansika Weerasena'\n",
    "EMAIL = 'hansikam.lokukat@ufl.edu'\n",
    "UFID = 11639514\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print('Homework 5 -- name: {}, email: {}, UFID: {}\\n'.format(NAME, EMAIL, UFID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c41a13a5f6e2fa1b47621d28e926c2cf",
     "grade": true,
     "grade_id": "name-email-ufid-asserts",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check that your name, email, and UFID is filled in.\"\"\"\n",
    "assert NAME != '' and NAME != 'Your name here.' and len(NAME) > 3\n",
    "assert EMAIL != '' and EMAIL != 'Your email here.' and len(EMAIL) > 7\n",
    "assert type(UFID) == int and UFID != 12345678 and UFID >= 10000000 and UFID <= 99999999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b4f30c4bbc2f9927b1410eeb1f001b3",
     "grade": false,
     "grade_id": "preamble-academic-integrity",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Academic Integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99184dabc791053131230787b9f498b8",
     "grade": false,
     "grade_id": "preamble-academic-integrity-2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### <span style=\"color:red;\">This is an individual assignment. Academic integrity violations (i.e., cheating, plagiarism) will be reported to SCCR!</span><br/>\n",
    "#### The official CISE policy recommended for such offenses is a course grade of E. Additional sanctions may be imposed by SCCR such as marks on your permanent educational transcripts, dismissal or expulsion.\n",
    "#### Reminder of the Honor Pledge: On all work submitted for credit by Students at the University of Florida, the following pledge is either required or implied: *\"On my honor, I have neither given nor received unauthorized aid in doing this assignment.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5d8d2452e1d9f6d547eae6447b7ca369",
     "grade": false,
     "grade_id": "cell-preamble-academic-integrity-3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Acknowledgement: Do you acknowledge and understand the academic integrity warning above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "89bc9ed2e09cb9069b92dc24a3bc081a",
     "grade": false,
     "grade_id": "academic-integrity",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "academic_integrity_acknowledgement = False\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7d6eb103ab3a60e964c163468d9aa7a",
     "grade": true,
     "grade_id": "academic-integrity-assert",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check that you acknowledge the academic integrity warning, you understand it and have been reminded of the UF Honor Pledge.\"\"\"\n",
    "assert academic_integrity_acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "40e0c6343e3d6ae8ef5568125e9de64f",
     "grade": false,
     "grade_id": "task1-instructc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The following cell's code (import statements etc.) is provided for you and you should not need to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b90e91ac559d6215dd661358acd69605",
     "grade": false,
     "grade_id": "task0-code",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Load packages we need\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist, cifar10, imdb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import layers and callbacks we may use (may not be a complete list)\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "from tensorflow.python.keras.utils import layer_utils\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "#%load_ext tensorboard\n",
    "\n",
    "\n",
    "# Let's check our software versions\n",
    "print('------------')\n",
    "print('### Python version: ' + __import__('sys').version)\n",
    "print('### NumPy version: ' + np.__version__)\n",
    "print('### Scikit-learn version: ' + sklearn.__version__)\n",
    "print('### Tensorflow version: ' + tf.__version__)\n",
    "print('------------')\n",
    "\n",
    "def var_exists(var_name):\n",
    "    return (var_name in globals() or var_name in locals())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cc6d3b909b447a027649ca1d83883d9",
     "grade": false,
     "grade_id": "seed_instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### This is the seed we will use, do not change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bba71034a59520e609f24b40475e074",
     "grade": false,
     "grade_id": "setting_seed",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "prop_vec = [16, 2, 2] # proportions for train - val - test splits\n",
    "\n",
    "epsf = 1e-9 # small epsilon value for floating point comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d746a78509c270bb1f51eff6a455a1f6",
     "grade": true,
     "grade_id": "seed_checking",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check seed. \"\"\"\n",
    "assert seed == 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5c2d9a55031f0ac22e54746a1a59624",
     "grade": false,
     "grade_id": "cell-e54b15b47353522a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Plots a set of images (all m x m)\n",
    "## input is  a square number of images, i.e., np.array with shape (z*z, dim_x, dim_y) for some integer z > 1\n",
    "\"\"\"\n",
    "def plot_images(im, dim_x=28, dim_y=28, one_row=False, out_fp='out.png', save=False, show=True, cmap='gray', fig_size=(14,14), titles=None, titles_fontsize=12):\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    im = im.reshape((-1, dim_x, dim_y))\n",
    "\n",
    "    num = im.shape[0]\n",
    "    assert num <= 3 or np.sqrt(num)**2 == num or one_row, 'Number of images is too large or not a perfect square!'\n",
    "    \n",
    "    if titles is not None:\n",
    "        assert num == len(titles)\n",
    "    \n",
    "    if num <= 3:\n",
    "        for i in range(0, num):\n",
    "            plt.subplot(1, num, 1 + i)\n",
    "            plt.axis('off')\n",
    "            if type(cmap) == list:\n",
    "                assert len(cmap) == num\n",
    "                plt.imshow(im[i], cmap=cmap[i]) # plot raw pixel data\n",
    "            else:\n",
    "                plt.imshow(im[i], cmap=cmap) # plot raw pixel data\n",
    "            if titles is not None:\n",
    "                plt.title(titles[i], fontsize=titles_fontsize)\n",
    "    else:\n",
    "        sq = int(np.sqrt(num))\n",
    "        for i in range(0, num):\n",
    "            if one_row:\n",
    "                plt.subplot(1, num, 1 + i)\n",
    "            else:\n",
    "                plt.subplot(sq, sq, 1 + i)\n",
    "            plt.axis('off')\n",
    "            if type(cmap) == list:\n",
    "                assert len(cmap) == num\n",
    "                plt.imshow(im[i], cmap=cmap[i]) # plot raw pixel data\n",
    "            else:\n",
    "                plt.imshow(im[i], cmap=cmap) # plot raw pixel data\n",
    "            if titles is not None:\n",
    "                plt.title(titles[i], fontsize=titles_fontsize)\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(out_fp)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "05666a91fba157e0dcf379a12b3a6805",
     "grade": false,
     "grade_id": "task0-instructd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Loading CIFAR-10 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa6895ae4e2bbee1e0eecc5492a925e9",
     "grade": false,
     "grade_id": "task0-loaddata-test",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# refer to: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10/load_data\n",
    "# and to https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "def load_preprocess_cifar10(onehot=True, minmax_normalize=True):\n",
    "    labels = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'])\n",
    "    train, testval = cifar10.load_data()\n",
    "    \n",
    "    train_x, train_y = train\n",
    "    testval_x, testval_y = testval\n",
    "    \n",
    "    if minmax_normalize:\n",
    "        train_x = train_x / 255.0\n",
    "        testval_x = testval_x / 255.0\n",
    "        \n",
    "    if onehot:\n",
    "        train_y = keras.utils.to_categorical(train_y, labels.shape[0])\n",
    "        testval_y = keras.utils.to_categorical(testval_y, labels.shape[0])\n",
    "    \n",
    "    # split test - val\n",
    "    nval = testval_x.shape[0] // 2\n",
    "    \n",
    "    val_x = testval_x[:nval]\n",
    "    val_y = testval_y[:nval]\n",
    "    \n",
    "    test_x = testval_x[nval:]\n",
    "    test_y = testval_y[nval:]\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a641c793abb77f6554579d7d48951137",
     "grade": false,
     "grade_id": "task0-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# do some sanity checks\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10(onehot=False, minmax_normalize=False)\n",
    "assert train_x.shape[0] == train_y.shape[0] and test_x.shape[0] == test_y.shape[0] and val_x.shape[0] == val_y.shape[0]\n",
    "assert np.amax(train_x) >= 255 and np.amax(test_x) >= 255 and np.amax(val_x) >= 255\n",
    "assert train_y.shape == (train_y.shape[0],) or train_y.shape == (train_y.shape[0],1)\n",
    "\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10(onehot=True, minmax_normalize=False)\n",
    "assert np.amax(train_x) >= 255 and np.amax(test_x) >= 255 and np.amax(val_x) >= 255\n",
    "assert train_y.shape == (train_y.shape[0],10) and train_y.shape[1] == test_y.shape[1]\n",
    "\n",
    "\n",
    "# actually load the data\n",
    "train_x, train_y, test_x, test_y, val_x, val_y, labels = load_preprocess_cifar10()\n",
    "assert np.amax(train_x) <= 1 and np.amax(test_x) <= 1 and np.amax(val_x) <= 1\n",
    "assert np.amax(train_x) >= 0 and np.amax(test_x) >= 0 and np.amax(val_x) >= 0\n",
    "\n",
    "assert labels.shape[0] == 10 and labels.shape[0] == train_y.shape[1]\n",
    "\n",
    "print(train_x.shape, val_x.shape, test_x.shape, train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "430c72516195eec0c84d1b0afa0e614b",
     "grade": false,
     "grade_id": "task1-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 1] (25 points) Training a CNN For CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jp-MarkdownHeadingCollapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be4490efcca698dd92993c58a8ffca3d",
     "grade": false,
     "grade_id": "task1-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### We will use the following architecture\n",
    "- Conv layer with 32 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 32 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Conv layer with 64 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 64 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Conv layer with 128 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Conv layer with 128 filters, (3,3) filter size, stride of 1, padding 'same'\n",
    "- Max pooling layer (2,2)\n",
    "- Flatten\n",
    "- FC with 128 units\n",
    "- Dropout with rate 25%\n",
    "- FC with 96 units\n",
    "- Dropout with rate 25%\n",
    "- (Output layer) FC with 10 units\n",
    "\n",
    "#### For all layers (if applicable) except the output layer you should use:\n",
    "- ReLU as activation function\n",
    "- LeCun uniform weight initialization strategy\n",
    "- L2 regularization with regularization constant set to 0.001\n",
    "\n",
    "#### For the output layer you should select a suitable activation function that is consistent with the task and loss function you use. Use Adam for the optimizer with learning rate 0.002."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d370ed03690969541dc58703ed27b965",
     "grade": false,
     "grade_id": "task1a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 1a] (15 points) Implement create_compile_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5930de6b73eac87d49c657f12a843647",
     "grade": false,
     "grade_id": "task1a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_compile_cnn(input_shape=[32, 32, 3], num_outputs=10, verbose=False):\n",
    "    \n",
    "    model = keras.models.Sequential(name='CIFAR-10--CNN')\n",
    "    \n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    \"\"\"Fill in your code in the function (~20 lines)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0912d992d9cbbe1191f8be3d2eb97e7f",
     "grade": false,
     "grade_id": "cell-c6daf23a1d5c2162",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "_ = create_compile_cnn(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5b97ad67a2c630f5c674f4af9c910102",
     "grade": false,
     "grade_id": "task1b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 1b] (10 points) Train the model. Fill in the implementation below. \n",
    "#### Note: make sure to delete the model file on disk if incorrect, or else the assertions passing may not reflect a correct implementation (loading the model if it exists is there to save time and not retrain the model each time if the implementation does not change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "02855db6ed50e42fd54839642c262d07",
     "grade": false,
     "grade_id": "task1b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cnn_model_fp = './cifar10-cnn.tf'\n",
    "\n",
    "# If the model file exists, load it. Otherwise train it and save the model.\n",
    "# Note: if you need to retrain the model, simply delete the h5 file.\n",
    "if os.path.exists(cnn_model_fp):\n",
    "    model = keras.models.load_model(cnn_model_fp)\n",
    "else:\n",
    "    model = create_compile_cnn(verbose=False)\n",
    "    \n",
    "    # do the training using model.fit() for at least 5 epochs and your chosen batch_size\n",
    "    # (make sure your network is actually learning...)\n",
    "    # you can set any callback you want on it, including checkpoint, early stopping, etc.\n",
    "    \"\"\"Fill in your code here (~3-5 lines)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # save the model\n",
    "    model.save(cnn_model_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dad691aca32b9e9331a3b74dad04883",
     "grade": true,
     "grade_id": "task1b-tests",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 1a and 1b completed. \"\"\"\n",
    "\n",
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))\n",
    "\n",
    "assert var_exists('model')\n",
    "\n",
    "model = create_compile_cnn(verbose=False) # recreate the model object in case the version on disk is no good\n",
    "\n",
    "trainable_count = layer_utils.count_params(model.trainable_weights)\n",
    "assert trainable_count > 500000 and trainable_count < 600000\n",
    "\n",
    "assert len(model.layers) == 15\n",
    "assert model.layers[0].filters == 32 and model.layers[0].groups == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e0505e5183d3bc838d1afdba2175497",
     "grade": false,
     "grade_id": "task2-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 2] (25 points) Processing Sequence Data with RNNs for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7cdbd0f782dd25cd711cab9179140b0",
     "grade": false,
     "grade_id": "task2-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## We will use the imdb dataset for this task. The dataset consists of reviews from IMDB. We will use it for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bbf774ff80a165c0cede8b600def1246",
     "grade": false,
     "grade_id": "task2a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## [Task 2a] (10 points) Fill in the implementation of load_preprocess_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "555ce5bf3992d9f03363b80c76e033b3",
     "grade": false,
     "grade_id": "task2a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# refer to: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb\n",
    "def load_preprocess_imdb(num_words=10000, train_prop=0.8, val_prop=0.5, maxlen=100, vectorize=False):\n",
    "\n",
    "    train, testval = imdb.load_data(num_words=num_words, maxlen=maxlen, oov_char=0)\n",
    "    \n",
    "    ### Process the data \n",
    "    ### Merge train and testval ('all_x', 'all_y')\n",
    "    ### then you must encode the features of each examples as a sequence of size maxlen (represented as a np.array). You can use np.pad to pad the sequence. \n",
    "    ### Make sure to pad sequences with 0 as appropriate.\n",
    "    \"\"\"Fill in your code here (~5-10 lines)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # split the data into train, test, val\n",
    "    # do the split\n",
    "    train_x, testval_x, train_y, testval_y = train_test_split(all_x, all_y, test_size=1-train_prop, random_state=seed)\n",
    "    val_x, test_x, val_y, test_y = train_test_split(testval_x, testval_y, test_size=1-val_prop, random_state=seed)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d7014c6ff13ffc3708df42c25a7ea5c2",
     "grade": false,
     "grade_id": "cell-a5742ef4a313068b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# the size of the vocabulary we'll use\n",
    "vocab_size = 12000\n",
    "maxlen = 150\n",
    "\n",
    "# sanity checks\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_imdb(num_words=vocab_size, maxlen=maxlen)\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3602448cf1a11387fc0fd9f9b9004be",
     "grade": true,
     "grade_id": "task2a-tests",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 2a completed. \"\"\"\n",
    "\n",
    "assert var_exists('train_x') and var_exists('train_y')\n",
    "assert train_x.shape == (15195, maxlen) and train_y.shape == (train_x.shape[0],)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70494a4505eb077b4fe93cefd291efaf",
     "grade": false,
     "grade_id": "task2b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 2b] (15 points) Complete the code below to define an RNN architecture for sentiment analysis. The goal is to predict the sentiment of IMDB reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f941267151938cb5ba9bd8d45512f2f6",
     "grade": false,
     "grade_id": "task2b-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Use the following architecture. For whatever is not specified (loss function, metrics, return sequences, optimizer, learning rate, etc.) you should complete it as appropriate.\n",
    "- Embedding layer of size 'embedding_size'\n",
    "- GRU layer with 64 units\n",
    "- GRU layer with 64 units and dropout rate 20%\n",
    "- Dense layer with num_outputs units\n",
    "\n",
    "### With this architecture correctly implemented you should have about 2.4m parameters and it should be easy to achieve 88%+ test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "880566720968f065c685754944c214b0",
     "grade": false,
     "grade_id": "task2b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_compile_rnn(input_shape=[None], vocab_size=vocab_size, embedding_size=196, num_outputs=1, verbose=False):\n",
    "    \n",
    "    model = keras.models.Sequential(name='imdb-RNN')\n",
    "    \n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    \"\"\"Fill in your code here (~5-10 lines)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6028aceec6bac3eaf9b94af1c62a5bb",
     "grade": false,
     "grade_id": "cell-acc703ed78820664",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = create_compile_rnn(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a9aa69ada7ce9b8428691d48a19893c",
     "grade": false,
     "grade_id": "task2b-code2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "early_stop_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# feel free to tweak the batch size, number of epochs and callbacks.\n",
    "max_epochs = 5\n",
    "batch_size = 128\n",
    "\n",
    "\"\"\"Fill in your code here (~1-2 lines)\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size, validation_data=(val_x, val_y), callbacks=[early_stop_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a55eca7e6b89d7bf8f49bf3682d5940e",
     "grade": true,
     "grade_id": "task2b-checks",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 2b completed. \"\"\"\n",
    "\n",
    "assert var_exists('model') and var_exists('hist')\n",
    "val_acc = hist.history['val_accuracy']\n",
    "assert np.amax(val_acc) >= 0.875\n",
    "\n",
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print('[Model] Test accuracy: {:.2f}%'.format(100*acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9712be01658be8bab975541caf1666a4",
     "grade": false,
     "grade_id": "task3-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 3] (25 points) Sentiment Analysis with Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2284b541a47e8c8898cc57fb741dc3ba",
     "grade": false,
     "grade_id": "task3-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## In this task the objective is to use multi-head attention (layers.MultiHeadAttention) to match or exceed the performance of the RNN trained in Task 2.\n",
    "## You can use whatever architecture you want as long as it includes at least one MultiHeadAttention layer (with at least two heads). Ideally we also don't want it to be an RNN.\n",
    "## The catch is we want to do it using much fewer total parameters.\n",
    "## For example, we will aim to use less than 500k trainable parameters while still achieving 88%+ test/val accuracy.\n",
    "## You will also need to ensure the total training time is less than 10 minutes.\n",
    "\n",
    "## Hints: \n",
    "- you may find it useful to use the Function API of Keras.\n",
    "- a possible approach is to use an embedding layer appropriately connected to the MultiHeadAttention, but note that if the embedding layer cannot be too large or it will use up too many parameters. \n",
    "- you may also find it useful to use dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5754eab01c266b733d8b3a3bb4258f8b",
     "grade": false,
     "grade_id": "task3-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_compile_attention(input_shape=(150,), vocab_size=vocab_size, num_outputs=1, verbose=False):\n",
    "    \n",
    "    ### Don't forget to compile the model and print the summary if verbose=True\n",
    "    \"\"\"Fill in your code here (~10-15 lines). Set the name of your model to be imdb-Attention'\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d275bcb56d7205a23f5a1134965b684d",
     "grade": false,
     "grade_id": "cell-44626805cbb5ff41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model = create_compile_attention(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f64fa2a4894b4de847979b02275c75f8",
     "grade": false,
     "grade_id": "task3-code2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "early_stop_cb = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# feel free to tweak the batch size, number of epochs and callbacks.\n",
    "max_epochs = 5\n",
    "batch_size = 64\n",
    "\n",
    "\"\"\"Fill in your code here (~1-2 lines)\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "hist = model.fit(train_x, train_y, epochs=max_epochs, batch_size=batch_size, validation_data=(val_x, val_y), callbacks=[early_stop_cb])\n",
    "\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print('Total training time {:.1f} seconds'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c0f6ee944949f4e544eb4a8838f8c680",
     "grade": true,
     "grade_id": "task3-tests",
     "locked": true,
     "points": 25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 3 completed. \"\"\"\n",
    "\n",
    "assert var_exists('model') and var_exists('hist')\n",
    "assert model.name == 'imdb-Attention'\n",
    "\n",
    "val_acc = hist.history['val_accuracy']\n",
    "assert np.amax(val_acc) >= 0.87 # check that we match the RNN performance\n",
    "\n",
    "# check number of parameters \n",
    "trainable_count = layer_utils.count_params(model.trainable_weights)\n",
    "assert trainable_count <= 500000\n",
    "\n",
    "# check total training time\n",
    "assert elapsed_time <= 600\n",
    "\n",
    "# let's evaluate the model on the test data\n",
    "loss, acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "acc_per_100kparam_score = acc / (trainable_count / (100*1000))\n",
    "print('[Model] Test accuracy: {:.2f}% [score: {:.2f}]'.format(100*acc, 100*acc_per_100kparam_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a788de157f6eedd094a60355ae724fa9",
     "grade": false,
     "grade_id": "cell-aed50e341ba2d8e5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### How high of a score did you achieve? (It is possible to match the RNN performance and easily achieve scores above 100.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eca7123b52614d60a9337c79efe089fc",
     "grade": false,
     "grade_id": "task4-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 4] (25 points) AutoEncoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ee10910dae49a2bc69011f0614a7f80",
     "grade": false,
     "grade_id": "task4-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### We will use Fashion-MNIST for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4f695d5ec533ff92879d66c0b314497",
     "grade": false,
     "grade_id": "task4-provided",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "## Load and preprocess the Fashion-MNIST dataset\n",
    "\"\"\"\n",
    "def load_preprocess_fmnist_data(flatten=True, onehot=True, minmax_normalize=True, val_prop=0.5, seed=None, verbose=False):\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    \n",
    "    assert x_train.shape == (60000, 28, 28) and x_test.shape == (10000, 28, 28)\n",
    "    assert y_train.shape == (60000,) and y_test.shape == (10000,)\n",
    "\n",
    "    if minmax_normalize:\n",
    "        x_train = x_train / 255.0\n",
    "        x_test = x_test / 255.0\n",
    "\n",
    "    if verbose: \n",
    "        print('Loaded Fashion-MNIST data; shape: {} [y: {}], test shape: {} [y: {}]'.format(x_train.shape, y_train.shape,\n",
    "                                                                                      x_test.shape, y_test.shape))\n",
    "    \n",
    "    if flatten: # Let's flatten the images for easier processing (labels don't change)\n",
    "        flat_vec_size = 28*28\n",
    "        x_train = x_train.reshape(x_train.shape[0], flat_vec_size)\n",
    "        x_test = x_test.reshape(x_test.shape[0], flat_vec_size)\n",
    "\n",
    "    if onehot: # Put the labels in \"one-hot\" encoding using keras' to_categorical()\n",
    "        num_classes = 10\n",
    "        y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "        y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    train_x = x_train\n",
    "    train_y = y_train\n",
    "\n",
    "    # we'll split the test set into test and val\n",
    "    testval_x = x_test\n",
    "    testval_y = y_test\n",
    "\n",
    "    # do the split\n",
    "    val_x, test_x, val_y, test_y = train_test_split(testval_x, testval_y, test_size=val_prop, random_state=seed)\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, val_x, val_y\n",
    "\n",
    "\n",
    "# grab the data\n",
    "train_x, train_y, test_x, test_y, val_x, val_y = load_preprocess_fmnist_data(flatten=False, onehot=False, val_prop=0.5, seed=seed) \n",
    "\n",
    "# sanity check shapes\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape, val_x.shape, val_y.shape)\n",
    "\n",
    "assert train_x.shape == (60000, 28, 28) and train_y.shape == (60000,) and test_x.shape == (5000, 28, 28) and test_y.shape == (5000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c492e7bf019810f8f821d1466747ee1c",
     "grade": false,
     "grade_id": "task4a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 4a] (15 points) Complete the implementation of create_simple_ae() according to the architecture and instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0f4286df895554f90d791b2bf2651221",
     "grade": false,
     "grade_id": "task4a-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Start by creating the encoder ('enc_model'). It should have the following architecture:\n",
    "- Flatten\n",
    "- FC with hidden_widths[0] units with activation ReLU\n",
    "- FC with hidden_widths[1] units with activation ReLU\n",
    "- Dropout with rate 'dropout_rate'\n",
    "- FC with latent_width units with activation *sigmoid*\n",
    "\n",
    "#### Then create the decoder ('dec_model'). It should have the following architecture:\n",
    "- (Input has shape latent_width)\n",
    "- FC with hidden_widths[1] units with activation ReLU\n",
    "- FC with hidden_widths[0] units with activation ReLU\n",
    "- Dropout with rate 'dropout_rate'\n",
    "- FC with 784 units with activation *sigmoid*\n",
    "- Reshape to input_shape\n",
    "\n",
    "#### Finally connect the two models together into a new model ('ae_model'). Make sure that the model takes the input that the encoder takes and produces the output that the decoder produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06e90e142f3b191e667ad18ee4389546",
     "grade": false,
     "grade_id": "task4a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_simple_ae(input_shape=(28, 28), latent_width=64, hidden_widths=[256, 96], dropout_rate=0.0, verbose=False):\n",
    "    name = 'AE-simple'\n",
    "\n",
    "    # encoder\n",
    "    enc_input = Input(shape=input_shape, name='Encoder-input')\n",
    "    \"\"\"Fill in your code here (~3-5 lines)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    enc_model = Model(inputs=[enc_input], outputs=[enc_output], name='Encoder')\n",
    "   \n",
    "    \n",
    "    # decoder\n",
    "    dec_input = Input(shape=[latent_width], name='Decoder-input')\n",
    "    \"\"\"Fill in your code here (~3-5 lines)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    dec_model = Model(inputs=[dec_input], outputs=[dec_reshape], name='Decoder')\n",
    "    \n",
    "    # connect the two\n",
    "    \"\"\"Fill in your code here (~3-5 lines)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    opt = keras.optimizers.Adam(learning_rate=0.003)\n",
    "    \n",
    "    if verbose:\n",
    "        ae_model.summary()\n",
    "    \n",
    "    ae_model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['mse'])\n",
    "    \n",
    "    return name, ae_model, enc_model, dec_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9801e8c30da06ec9874c4b6ce92126d5",
     "grade": false,
     "grade_id": "cell-3a338b8d447fe6f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Let's train the model -- you should get a loss lower than 0.28 and MSE lower than 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f8c2d447e8c0cec8eea40d6b90f6831f",
     "grade": false,
     "grade_id": "cell-6011d47441ac22f0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's train the model (no need to change any of this).\n",
    "latent_width = 64\n",
    "name, ae_model, enc_model, dec_model = create_simple_ae(latent_width=latent_width, verbose=True)\n",
    "    \n",
    "max_epochs = 40\n",
    "batch_size = 96\n",
    "\n",
    "hist = ae_model.fit(train_x, train_x, epochs=max_epochs, batch_size=batch_size, validation_data=(val_x, val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3edb070acf9f15d1931b30e557fd8cb",
     "grade": true,
     "grade_id": "task4a-tests",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 4a completed. \"\"\"\n",
    "\n",
    "assert var_exists('hist')\n",
    "train_loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "val_mse = hist.history['val_mse']\n",
    "\n",
    "assert train_loss[-1] < val_loss[-1]\n",
    "assert val_loss[-1] < 0.28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be779d0d165713d11958ffe5f212bfaa",
     "grade": false,
     "grade_id": "task4b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 4b] (10 points) We will use plot_images() to plot the first 81 data points in the val_x. Then reconstruct val_x through the auto-encoder and plot the first 81 data points of the reconstructed data. Fill in the code in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30d28dfa9f01431f0371c7e8a71b6ef4",
     "grade": false,
     "grade_id": "task4b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Put the first 81 data points of val_x in 'val_x_sel'\n",
    "\"\"\"Fill in your code here (~1 line)\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "plot_images(val_x_sel, dim_x=28, dim_y=28, fig_size=(10,10))\n",
    "\n",
    "# Run the data points in 'val_x_sel' through the auto-encoder to obtain 'rec_val_x_sel'\n",
    "\"\"\"Fill in your code here (~1 line)\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Plot the reconstructed data\n",
    "plot_images(rec_val_x_sel, dim_x=28, dim_y=28, fig_size=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6baa967235c4f590b295f27492d422a1",
     "grade": false,
     "grade_id": "cell-1d437e62b6b8d575",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Reconstructions should look pretty good but also a bit blurry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ca5996cec11b7e4e023f2349255164b",
     "grade": true,
     "grade_id": "task4b-tests",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 4b completed. \"\"\"\n",
    "\n",
    "assert var_exists('val_x_sel') and var_exists('rec_val_x_sel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5b3e997e5f1a1807ba34e7bd4a7911b",
     "grade": false,
     "grade_id": "task5-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---\n",
    "# [Task 5] \\<*For CAI6108MLE Only*\\> (25 points) AutoEncoder Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9fa07b1aea0834cda75910e0b260f4fb",
     "grade": false,
     "grade_id": "task5-instruct2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## In this task we will explore the latent space of the autocoder we have trained in Task 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b1d97c34a92c3d0464da13d1655ad5c",
     "grade": false,
     "grade_id": "task5a-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 5a] (10 points) Exploring the latent space. Compute the average latent space representation of tops and bags using the validation data. Call these 'avg_latent_tops' and 'avg_latent_bags' respectively. Then fill in the provided code so it plots examples along the interpolation path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5abd009ae069cb26b8fe157504002ccb",
     "grade": false,
     "grade_id": "task5a-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_exploration(source, target, num_steps):\n",
    "    diff = target - source\n",
    "    inc = diff / num_steps\n",
    "    ret = np.zeros((num_steps, source.shape[0]))\n",
    "    for i in range(0, num_steps):\n",
    "        ret[i] = source + inc * i\n",
    "    return ret\n",
    "\n",
    "labels = ['Top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# first compute the average latent representation\n",
    "\"\"\"Fill in your code here (~5-7 lines)\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Now let's compute and plot along the interpolation path. \n",
    "num_points = 81\n",
    "expl = linear_exploration(avg_tops_latent, avg_bags_latent, num_points)\n",
    "\"\"\"Fill in your code here (~1 lines). Store the results in 'expl_images'\n",
    "\"\"\"\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "plot_images(expl_images.reshape(-1, 28, 28), dim_x=28, dim_y=28, fig_size=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d2c0d06af420646e87a8ca33b62832d5",
     "grade": true,
     "grade_id": "task5a-tests",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" [ASSERTS] Check 5a completed. \"\"\"\n",
    "\n",
    "assert var_exists('avg_tops_latent') and var_exists('avg_bags_latent')\n",
    "assert avg_tops_latent.shape == (latent_width,) or avg_tops_latent.shape == (latent_width,1)\n",
    "assert avg_bags_latent.shape == (latent_width,) or avg_bags_latent.shape == (latent_width,1)\n",
    "\n",
    "assert var_exists('expl_images')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53316005cbb5ff9a1176d37ef5b5b294",
     "grade": false,
     "grade_id": "task5b-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 5b] (5 points) Complete the implementation of sample_new() that will sample new data points from the uniform [0,1] distribution over the latent space. You should *not* use any data (train_x/y, val_x/y, test_x/y, etc.). The provided code will then sample new images. How do they look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ccdbddd96ab4e3aa878cb2e6574c1e6",
     "grade": false,
     "grade_id": "task5b-code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Fill in your code here (~1-2 lines)\n",
    "\"\"\"\n",
    "def sample_new(dec_model, count, latent_width=latent_width):\n",
    "    ### sample 'count' new latent space points, decode them and return the result.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "# generate 81 new images\n",
    "sample_images = sample_new(dec_model, 81, latent_width=latent_width)\n",
    "\n",
    "# plot the produced images\n",
    "plot_images(sample_images.reshape(-1, 28, 28), dim_x=28, dim_y=28, fig_size=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "768872bdddd3c0315c50311f10ea932b",
     "grade": false,
     "grade_id": "task5c-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 5c] (5 points) Do the images look like fashion-MNIST images? If not, explain why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82c08ef7174ab99c7d9f7e61eac8ce3c",
     "grade": true,
     "grade_id": "task5c-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee1677d4debbf849cf90d91de76d4fbf",
     "grade": false,
     "grade_id": "cell-6a67124cf050fa10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The following code will train the autoencoder again but this time with dropout=0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f86732ea381630e4b2adc9a1b0fb75fc",
     "grade": true,
     "grade_id": "task5b-tests",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's train the model\n",
    "latent_width = 64\n",
    "del ae_model, enc_model, dec_model\n",
    "name, ae_model, enc_model, dec_model = create_simple_ae(latent_width=latent_width, dropout_rate=0.2, verbose=True)\n",
    "    \n",
    "max_epochs = 30\n",
    "batch_size = 96\n",
    "\n",
    "hist = ae_model.fit(train_x, train_x, epochs=max_epochs, batch_size=batch_size, validation_data=(val_x, val_x))\n",
    "\n",
    "\"\"\" [ASSERTS] \"\"\"\n",
    "assert var_exists('hist')\n",
    "val_mse = hist.history['val_mse']\n",
    "\n",
    "assert val_mse[-1] >= 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "564dec58f4f0425ed31485f72c8a85c5",
     "grade": false,
     "grade_id": "cell-fbe98bdc8636576c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### We will now generate new images with the (newly trained) decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3139312ad25e9738ad735131d92a37fa",
     "grade": false,
     "grade_id": "cell-443ec8cac75b7101",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# generate 81 new images\n",
    "sample_images = sample_new(dec_model, 81, latent_width=latent_width)\n",
    "\n",
    "# plot the produced images\n",
    "plot_images(sample_images.reshape(-1, 28, 28), dim_x=28, dim_y=28, fig_size=(10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "077c1014b36a498b81ac548ec9e75139",
     "grade": false,
     "grade_id": "task5d-instruct",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [Task 5d] (5 points) Do these new images look more like fashion-MNIST images than before? If yes, explain why. Why does including dropout have this effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cf640848318d4c997e7456727f6360da",
     "grade": true,
     "grade_id": "task5d-answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Remove the 'raise NotImplementedError' line(s). Write your answer as a comment in the place provided.  (Do not change the cell type from code to markdown.)\"\"\"\n",
    "# \n",
    "## Answer: \n",
    "#\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
